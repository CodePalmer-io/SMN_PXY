-- t=000 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0100) => 0.0000
activation[4] = self(0.0000) * weight[4,4](0.7000) => 0.0000
activation[5] = self(0.0000) * weight[5,5](0.1000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + output[1](0.0000) * 1.0000 => 0.0000
activation[3] = self(0.0000) + output[2](0.0000) * -0.8000 => 0.0000
activation[3] = self(0.0000) + output[5](0.0000) * -0.8000 => 0.0000
activation[4] = self(0.0000) + output[2](0.0000) * -0.6000 => 0.0000
activation[4] = self(0.0000) + output[3](0.0000) * 0.9000 => 0.0000
activation[5] = self(0.0000) + output[2](0.0000) * -0.8000 => 0.0000
activation[5] = self(0.0000) + output[3](0.0000) * -0.7000 => 0.0000
activation[5] = self(0.0000) + output[4](0.0000) * 0.5000 => 0.0000
activation[6] = self(0.0000) + output[2](0.0000) * 0.5000 => 0.0000
activation[7] = self(0.0000) + output[3](0.0000) * 1.0000 => 0.0000
activation[8] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5000)
networkOutput[2] := neuronOutput[8](0.5000)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](0.9000) / sum(0.9000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](0.5000) / sum(0.5000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](0.5000) / sum(0.5000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=001 --
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.0100) => 0.0000
activation[4] = self(0.0000) * weight[4,4](0.7000) => 0.0000
activation[5] = self(0.0000) * weight[5,5](0.1000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + output[1](0.9933) * 1.0000 => 0.9933
activation[3] = self(0.9933) + output[2](0.5000) * -0.8000 => 0.5933
activation[3] = self(0.5933) + output[5](0.5000) * -0.8000 => 0.1933
activation[4] = self(0.0000) + output[2](0.5000) * -0.6000 => -0.3000
activation[4] = self(-0.3000) + output[3](0.5000) * 1.0000 => 0.2000
activation[5] = self(0.0000) + output[2](0.5000) * -0.8000 => -0.4000
activation[5] = self(-0.4000) + output[3](0.5000) * -0.7000 => -0.7500
activation[5] = self(-0.7500) + output[4](0.5000) * 1.0000 => -0.2500
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5000) * 1.0000 => 0.5000
activation[8] = self(0.0000) + output[5](0.5000) * 1.0000 => 0.5000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9241)
networkOutput[2] := neuronOutput[8](0.9241)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=002 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.1933) * weight[3,3](0.0100) => 0.0019
activation[4] = self(0.2000) * weight[4,4](0.7000) => 0.1400
activation[5] = self(-0.2500) * weight[5,5](0.1000) => -0.0250
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5000) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.5000) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0019) + output[1](0.5000) * 1.0000 => 0.5019
activation[3] = self(0.5019) + output[2](0.5000) * -0.8000 => 0.1019
activation[3] = self(0.1019) + output[5](0.2227) * -0.8000 => -0.0762
activation[4] = self(0.1400) + output[2](0.5000) * -0.6000 => -0.1600
activation[4] = self(-0.1600) + output[3](0.7244) * 1.0000 => 0.5644
activation[5] = self(-0.0250) + output[2](0.5000) * -0.8000 => -0.4250
activation[5] = self(-0.4250) + output[3](0.7244) * -0.7000 => -0.9321
activation[5] = self(-0.9321) + output[4](0.7311) * 1.0000 => -0.2010
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.7244) * 1.0000 => 0.7244
activation[8] = self(0.0000) + output[5](0.2227) * 1.0000 => 0.2227
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9740)
networkOutput[2] := neuronOutput[8](0.7528)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=003 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.0762) * weight[3,3](0.0100) => -0.0008
activation[4] = self(0.5644) * weight[4,4](0.7000) => 0.3951
activation[5] = self(-0.2010) * weight[5,5](0.1000) => -0.0201
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.7244) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.2227) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0008) + output[1](0.5000) * 1.0000 => 0.4992
activation[3] = self(0.4992) + output[2](0.5000) * -0.8000 => 0.0992
activation[3] = self(0.0992) + output[5](0.2679) * -0.8000 => -0.1151
activation[4] = self(0.3951) + output[2](0.5000) * -0.6000 => 0.0951
activation[4] = self(0.0951) + output[3](0.4059) * 1.0000 => 0.5010
activation[5] = self(-0.0201) + output[2](0.5000) * -0.8000 => -0.4201
activation[5] = self(-0.4201) + output[3](0.4059) * -0.7000 => -0.7042
activation[5] = self(-0.7042) + output[4](0.9439) * 1.0000 => 0.2397
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.4059) * 1.0000 => 0.4059
activation[8] = self(0.0000) + output[5](0.2679) * 1.0000 => 0.2679
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.8838)
networkOutput[2] := neuronOutput[8](0.7924)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=004 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.1151) * weight[3,3](0.0100) => -0.0012
activation[4] = self(0.5010) * weight[4,4](0.7000) => 0.3507
activation[5] = self(0.2397) * weight[5,5](0.1000) => 0.0240
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.4059) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.2679) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0012) + output[1](0.5000) * 1.0000 => 0.4988
activation[3] = self(0.4988) + output[2](0.5000) * -0.8000 => 0.0988
activation[3] = self(0.0988) + output[5](0.7682) * -0.8000 => -0.5157
activation[4] = self(0.3507) + output[2](0.5000) * -0.6000 => 0.0507
activation[4] = self(0.0507) + output[3](0.3600) * 1.0000 => 0.4106
activation[5] = self(0.0240) + output[2](0.5000) * -0.8000 => -0.3760
activation[5] = self(-0.3760) + output[3](0.3600) * -0.7000 => -0.6280
activation[5] = self(-0.6280) + output[4](0.9245) * 1.0000 => 0.2965
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.3600) * 1.0000 => 0.3600
activation[8] = self(0.0000) + output[5](0.7682) * 1.0000 => 0.7682
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.8581)
networkOutput[2] := neuronOutput[8](0.9790)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=005 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.5157) * weight[3,3](0.0100) => -0.0052
activation[4] = self(0.4106) * weight[4,4](0.7000) => 0.2874
activation[5] = self(0.2965) * weight[5,5](0.1000) => 0.0296
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.3600) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7682) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0052) + output[1](0.5000) * 1.0000 => 0.4948
activation[3] = self(0.4948) + output[2](0.5000) * -0.8000 => 0.0948
activation[3] = self(0.0948) + output[5](0.8149) * -0.8000 => -0.5571
activation[4] = self(0.2874) + output[2](0.5000) * -0.6000 => -0.0126
activation[4] = self(-0.0126) + output[3](0.0705) * 1.0000 => 0.0580
activation[5] = self(0.0296) + output[2](0.5000) * -0.8000 => -0.3704
activation[5] = self(-0.3704) + output[3](0.0705) * -0.7000 => -0.4197
activation[5] = self(-0.4197) + output[4](0.8863) * 1.0000 => 0.4665
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0705) * 1.0000 => 0.0705
activation[8] = self(0.0000) + output[5](0.8149) * 1.0000 => 0.8149
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5873)
networkOutput[2] := neuronOutput[8](0.9833)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=006 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.5571) * weight[3,3](0.0100) => -0.0056
activation[4] = self(0.0580) * weight[4,4](0.7000) => 0.0406
activation[5] = self(0.4665) * weight[5,5](0.1000) => 0.0467
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0705) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.8149) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0056) + output[1](0.5000) * 1.0000 => 0.4944
activation[3] = self(0.4944) + output[2](0.5000) * -0.8000 => 0.0944
activation[3] = self(0.0944) + output[5](0.9116) * -0.8000 => -0.6348
activation[4] = self(0.0406) + output[2](0.5000) * -0.6000 => -0.2594
activation[4] = self(-0.2594) + output[3](0.0581) * 1.0000 => -0.2013
activation[5] = self(0.0467) + output[2](0.5000) * -0.8000 => -0.3533
activation[5] = self(-0.3533) + output[3](0.0581) * -0.7000 => -0.3940
activation[5] = self(-0.3940) + output[4](0.5720) * 1.0000 => 0.1779
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0581) * 1.0000 => 0.0581
activation[8] = self(0.0000) + output[5](0.9116) * 1.0000 => 0.9116
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5721)
networkOutput[2] := neuronOutput[8](0.9896)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=007 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.6348) * weight[3,3](0.0100) => -0.0063
activation[4] = self(-0.2013) * weight[4,4](0.7000) => -0.1409
activation[5] = self(0.1779) * weight[5,5](0.1000) => 0.0178
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0581) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.9116) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0063) + output[1](0.5000) * 1.0000 => 0.4937
activation[3] = self(0.4937) + output[2](0.5000) * -0.8000 => 0.0937
activation[3] = self(0.0937) + output[5](0.7088) * -0.8000 => -0.4734
activation[4] = self(-0.1409) + output[2](0.5000) * -0.6000 => -0.4409
activation[4] = self(-0.4409) + output[3](0.0402) * 1.0000 => -0.4008
activation[5] = self(0.0178) + output[2](0.5000) * -0.8000 => -0.3822
activation[5] = self(-0.3822) + output[3](0.0402) * -0.7000 => -0.4103
activation[5] = self(-0.4103) + output[4](0.2677) * 1.0000 => -0.1427
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0402) * 1.0000 => 0.0402
activation[8] = self(0.0000) + output[5](0.7088) * 1.0000 => 0.7088
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5500)
networkOutput[2] := neuronOutput[8](0.9719)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=008 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.4734) * weight[3,3](0.0100) => -0.0047
activation[4] = self(-0.4008) * weight[4,4](0.7000) => -0.2805
activation[5] = self(-0.1427) * weight[5,5](0.1000) => -0.0143
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0402) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7088) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0047) + output[1](0.5000) * 1.0000 => 0.4953
activation[3] = self(0.4953) + output[2](0.5000) * -0.8000 => 0.0953
activation[3] = self(0.0953) + output[5](0.3289) * -0.8000 => -0.1678
activation[4] = self(-0.2805) + output[2](0.5000) * -0.6000 => -0.5805
activation[4] = self(-0.5805) + output[3](0.0857) * 1.0000 => -0.4948
activation[5] = self(-0.0143) + output[2](0.5000) * -0.8000 => -0.4143
activation[5] = self(-0.4143) + output[3](0.0857) * -0.7000 => -0.4743
activation[5] = self(-0.4743) + output[4](0.1188) * 1.0000 => -0.3555
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0857) * 1.0000 => 0.0857
activation[8] = self(0.0000) + output[5](0.3289) * 1.0000 => 0.3289
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.6055)
networkOutput[2] := neuronOutput[8](0.8381)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=009 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.1678) * weight[3,3](0.0100) => -0.0017
activation[4] = self(-0.4948) * weight[4,4](0.7000) => -0.3464
activation[5] = self(-0.3555) * weight[5,5](0.1000) => -0.0355
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0857) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3289) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0017) + output[1](0.5000) * 1.0000 => 0.4983
activation[3] = self(0.4983) + output[2](0.5000) * -0.8000 => 0.0983
activation[3] = self(0.0983) + output[5](0.1446) * -0.8000 => -0.0174
activation[4] = self(-0.3464) + output[2](0.5000) * -0.6000 => -0.6464
activation[4] = self(-0.6464) + output[3](0.3017) * 1.0000 => -0.3447
activation[5] = self(-0.0355) + output[2](0.5000) * -0.8000 => -0.4355
activation[5] = self(-0.4355) + output[3](0.3017) * -0.7000 => -0.6467
activation[5] = self(-0.6467) + output[4](0.0777) * 1.0000 => -0.5690
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.3017) * 1.0000 => 0.3017
activation[8] = self(0.0000) + output[5](0.1446) * 1.0000 => 0.1446
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.8188)
networkOutput[2] := neuronOutput[8](0.6733)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=010 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.0174) * weight[3,3](0.0100) => -0.0002
activation[4] = self(-0.3447) * weight[4,4](0.7000) => -0.2413
activation[5] = self(-0.5690) * weight[5,5](0.1000) => -0.0569
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.3017) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.1446) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0002) + output[1](0.5000) * 1.0000 => 0.4998
activation[3] = self(0.4998) + output[2](0.5000) * -0.8000 => 0.0998
activation[3] = self(0.0998) + output[5](0.0549) * -0.8000 => 0.0559
activation[4] = self(-0.2413) + output[2](0.5000) * -0.6000 => -0.5413
activation[4] = self(-0.5413) + output[3](0.4783) * 1.0000 => -0.0630
activation[5] = self(-0.0569) + output[2](0.5000) * -0.8000 => -0.4569
activation[5] = self(-0.4569) + output[3](0.4783) * -0.7000 => -0.7917
activation[5] = self(-0.7917) + output[4](0.1514) * 1.0000 => -0.6403
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.4783) * 1.0000 => 0.4783
activation[8] = self(0.0000) + output[5](0.0549) * 1.0000 => 0.0549
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9162)
networkOutput[2] := neuronOutput[8](0.5682)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=011 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0559) * weight[3,3](0.0100) => 0.0006
activation[4] = self(-0.0630) * weight[4,4](0.7000) => -0.0441
activation[5] = self(-0.6403) * weight[5,5](0.1000) => -0.0640
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.4783) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0549) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0006) + output[1](0.5000) * 1.0000 => 0.5006
activation[3] = self(0.5006) + output[2](0.5000) * -0.8000 => 0.1006
activation[3] = self(0.1006) + output[5](0.0391) * -0.8000 => 0.0693
activation[4] = self(-0.0441) + output[2](0.5000) * -0.6000 => -0.3441
activation[4] = self(-0.3441) + output[3](0.5694) * 1.0000 => 0.2253
activation[5] = self(-0.0640) + output[2](0.5000) * -0.8000 => -0.4640
activation[5] = self(-0.4640) + output[3](0.5694) * -0.7000 => -0.8626
activation[5] = self(-0.8626) + output[4](0.4219) * 1.0000 => -0.4407
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5694) * 1.0000 => 0.5694
activation[8] = self(0.0000) + output[5](0.0391) * 1.0000 => 0.0391
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9452)
networkOutput[2] := neuronOutput[8](0.5487)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=012 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0693) * weight[3,3](0.0100) => 0.0007
activation[4] = self(0.2253) * weight[4,4](0.7000) => 0.1577
activation[5] = self(-0.4407) * weight[5,5](0.1000) => -0.0441
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5694) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0391) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0007) + output[1](0.5000) * 1.0000 => 0.5007
activation[3] = self(0.5007) + output[2](0.5000) * -0.8000 => 0.1007
activation[3] = self(0.1007) + output[5](0.0994) * -0.8000 => 0.0211
activation[4] = self(0.1577) + output[2](0.5000) * -0.6000 => -0.1423
activation[4] = self(-0.1423) + output[3](0.5857) * 1.0000 => 0.4434
activation[5] = self(-0.0441) + output[2](0.5000) * -0.8000 => -0.4441
activation[5] = self(-0.4441) + output[3](0.5857) * -0.7000 => -0.8541
activation[5] = self(-0.8541) + output[4](0.7552) * 1.0000 => -0.0989
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5857) * 1.0000 => 0.5857
activation[8] = self(0.0000) + output[5](0.0994) * 1.0000 => 0.0994
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9492)
networkOutput[2] := neuronOutput[8](0.6218)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=013 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0211) * weight[3,3](0.0100) => 0.0002
activation[4] = self(0.4434) * weight[4,4](0.7000) => 0.3104
activation[5] = self(-0.0989) * weight[5,5](0.1000) => -0.0099
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5857) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0994) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0002) + output[1](0.5000) * 1.0000 => 0.5002
activation[3] = self(0.5002) + output[2](0.5000) * -0.8000 => 0.1002
activation[3] = self(0.1002) + output[5](0.3789) * -0.8000 => -0.2029
activation[4] = self(0.3104) + output[2](0.5000) * -0.6000 => 0.0104
activation[4] = self(0.0104) + output[3](0.5264) * 1.0000 => 0.5368
activation[5] = self(-0.0099) + output[2](0.5000) * -0.8000 => -0.4099
activation[5] = self(-0.4099) + output[3](0.5264) * -0.7000 => -0.7784
activation[5] = self(-0.7784) + output[4](0.9018) * 1.0000 => 0.1234
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5264) * 1.0000 => 0.5264
activation[8] = self(0.0000) + output[5](0.3789) * 1.0000 => 0.3789
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9329)
networkOutput[2] := neuronOutput[8](0.8693)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=014 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.2029) * weight[3,3](0.0100) => -0.0020
activation[4] = self(0.5368) * weight[4,4](0.7000) => 0.3758
activation[5] = self(0.1234) * weight[5,5](0.1000) => 0.0123
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5264) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3789) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0020) + output[1](0.5000) * 1.0000 => 0.4980
activation[3] = self(0.4980) + output[2](0.5000) * -0.8000 => 0.0980
activation[3] = self(0.0980) + output[5](0.6496) * -0.8000 => -0.4217
activation[4] = self(0.3758) + output[2](0.5000) * -0.6000 => 0.0758
activation[4] = self(0.0758) + output[3](0.2661) * 1.0000 => 0.3419
activation[5] = self(0.0123) + output[2](0.5000) * -0.8000 => -0.3877
activation[5] = self(-0.3877) + output[3](0.2661) * -0.7000 => -0.5739
activation[5] = self(-0.5739) + output[4](0.9361) * 1.0000 => 0.3621
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.2661) * 1.0000 => 0.2661
activation[8] = self(0.0000) + output[5](0.6496) * 1.0000 => 0.6496
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.7909)
networkOutput[2] := neuronOutput[8](0.9626)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=015 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.4217) * weight[3,3](0.0100) => -0.0042
activation[4] = self(0.3419) * weight[4,4](0.7000) => 0.2393
activation[5] = self(0.3621) * weight[5,5](0.1000) => 0.0362
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.2661) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6496) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0042) + output[1](0.5000) * 1.0000 => 0.4958
activation[3] = self(0.4958) + output[2](0.5000) * -0.8000 => 0.0958
activation[3] = self(0.0958) + output[5](0.8594) * -0.8000 => -0.5918
activation[4] = self(0.2393) + output[2](0.5000) * -0.6000 => -0.0607
activation[4] = self(-0.0607) + output[3](0.1083) * 1.0000 => 0.0476
activation[5] = self(0.0362) + output[2](0.5000) * -0.8000 => -0.3638
activation[5] = self(-0.3638) + output[3](0.1083) * -0.7000 => -0.4396
activation[5] = self(-0.4396) + output[4](0.8468) * 1.0000 => 0.4072
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.1083) * 1.0000 => 0.1083
activation[8] = self(0.0000) + output[5](0.8594) * 1.0000 => 0.8594
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.6321)
networkOutput[2] := neuronOutput[8](0.9866)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=016 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.5918) * weight[3,3](0.0100) => -0.0059
activation[4] = self(0.0476) * weight[4,4](0.7000) => 0.0333
activation[5] = self(0.4072) * weight[5,5](0.1000) => 0.0407
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.1083) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.8594) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0059) + output[1](0.5000) * 1.0000 => 0.4941
activation[3] = self(0.4941) + output[2](0.5000) * -0.8000 => 0.0941
activation[3] = self(0.0941) + output[5](0.8845) * -0.8000 => -0.6135
activation[4] = self(0.0333) + output[2](0.5000) * -0.6000 => -0.2667
activation[4] = self(-0.2667) + output[3](0.0493) * 1.0000 => -0.2174
activation[5] = self(0.0407) + output[2](0.5000) * -0.8000 => -0.3593
activation[5] = self(-0.3593) + output[3](0.0493) * -0.7000 => -0.3938
activation[5] = self(-0.3938) + output[4](0.5592) * 1.0000 => 0.1654
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0493) * 1.0000 => 0.0493
activation[8] = self(0.0000) + output[5](0.8845) * 1.0000 => 0.8845
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5613)
networkOutput[2] := neuronOutput[8](0.9881)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=017 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.6135) * weight[3,3](0.0100) => -0.0061
activation[4] = self(-0.2174) * weight[4,4](0.7000) => -0.1522
activation[5] = self(0.1654) * weight[5,5](0.1000) => 0.0165
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0493) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.8845) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0061) + output[1](0.5000) * 1.0000 => 0.4939
activation[3] = self(0.4939) + output[2](0.5000) * -0.8000 => 0.0939
activation[3] = self(0.0939) + output[5](0.6957) * -0.8000 => -0.4627
activation[4] = self(-0.1522) + output[2](0.5000) * -0.6000 => -0.4522
activation[4] = self(-0.4522) + output[3](0.0445) * 1.0000 => -0.4077
activation[5] = self(0.0165) + output[2](0.5000) * -0.8000 => -0.3835
activation[5] = self(-0.3835) + output[3](0.0445) * -0.7000 => -0.4146
activation[5] = self(-0.4146) + output[4](0.2522) * 1.0000 => -0.1624
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0445) * 1.0000 => 0.0445
activation[8] = self(0.0000) + output[5](0.6957) * 1.0000 => 0.6957
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5553)
networkOutput[2] := neuronOutput[8](0.9701)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=018 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.4627) * weight[3,3](0.0100) => -0.0046
activation[4] = self(-0.4077) * weight[4,4](0.7000) => -0.2854
activation[5] = self(-0.1624) * weight[5,5](0.1000) => -0.0162
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0445) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6957) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0046) + output[1](0.5000) * 1.0000 => 0.4954
activation[3] = self(0.4954) + output[2](0.5000) * -0.8000 => 0.0954
activation[3] = self(0.0954) + output[5](0.3075) * -0.8000 => -0.1506
activation[4] = self(-0.2854) + output[2](0.5000) * -0.6000 => -0.5854
activation[4] = self(-0.5854) + output[3](0.0900) * 1.0000 => -0.4954
activation[5] = self(-0.0162) + output[2](0.5000) * -0.8000 => -0.4162
activation[5] = self(-0.4162) + output[3](0.0900) * -0.7000 => -0.4792
activation[5] = self(-0.4792) + output[4](0.1152) * 1.0000 => -0.3640
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0900) * 1.0000 => 0.0900
activation[8] = self(0.0000) + output[5](0.3075) * 1.0000 => 0.3075
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.6106)
networkOutput[2] := neuronOutput[8](0.8231)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=019 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.1506) * weight[3,3](0.0100) => -0.0015
activation[4] = self(-0.4954) * weight[4,4](0.7000) => -0.3468
activation[5] = self(-0.3640) * weight[5,5](0.1000) => -0.0364
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0900) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3075) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0015) + output[1](0.5000) * 1.0000 => 0.4985
activation[3] = self(0.4985) + output[2](0.5000) * -0.8000 => 0.0985
activation[3] = self(0.0985) + output[5](0.1394) * -0.8000 => -0.0130
activation[4] = self(-0.3468) + output[2](0.5000) * -0.6000 => -0.6468
activation[4] = self(-0.6468) + output[3](0.3201) * 1.0000 => -0.3266
activation[5] = self(-0.0364) + output[2](0.5000) * -0.8000 => -0.4364
activation[5] = self(-0.4364) + output[3](0.3201) * -0.7000 => -0.6605
activation[5] = self(-0.6605) + output[4](0.0775) * 1.0000 => -0.5830
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.3201) * 1.0000 => 0.3201
activation[8] = self(0.0000) + output[5](0.1394) * 1.0000 => 0.1394
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.8321)
networkOutput[2] := neuronOutput[8](0.6676)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=020 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.0130) * weight[3,3](0.0100) => -0.0001
activation[4] = self(-0.3266) * weight[4,4](0.7000) => -0.2286
activation[5] = self(-0.5830) * weight[5,5](0.1000) => -0.0583
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.3201) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.1394) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0001) + output[1](0.5000) * 1.0000 => 0.4999
activation[3] = self(0.4999) + output[2](0.5000) * -0.8000 => 0.0999
activation[3] = self(0.0999) + output[5](0.0514) * -0.8000 => 0.0587
activation[4] = self(-0.2286) + output[2](0.5000) * -0.6000 => -0.5286
activation[4] = self(-0.5286) + output[3](0.4837) * 1.0000 => -0.0450
activation[5] = self(-0.0583) + output[2](0.5000) * -0.8000 => -0.4583
activation[5] = self(-0.4583) + output[3](0.4837) * -0.7000 => -0.7969
activation[5] = self(-0.7969) + output[4](0.1634) * 1.0000 => -0.6335
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.4837) * 1.0000 => 0.4837
activation[8] = self(0.0000) + output[5](0.0514) * 1.0000 => 0.0514
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9182)
networkOutput[2] := neuronOutput[8](0.5639)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=021 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0587) * weight[3,3](0.0100) => 0.0006
activation[4] = self(-0.0450) * weight[4,4](0.7000) => -0.0315
activation[5] = self(-0.6335) * weight[5,5](0.1000) => -0.0633
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.4837) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0514) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0006) + output[1](0.5000) * 1.0000 => 0.5006
activation[3] = self(0.5006) + output[2](0.5000) * -0.8000 => 0.1006
activation[3] = self(0.1006) + output[5](0.0404) * -0.8000 => 0.0683
activation[4] = self(-0.0315) + output[2](0.5000) * -0.6000 => -0.3315
activation[4] = self(-0.3315) + output[3](0.5729) * 1.0000 => 0.2414
activation[5] = self(-0.0633) + output[2](0.5000) * -0.8000 => -0.4633
activation[5] = self(-0.4633) + output[3](0.5729) * -0.7000 => -0.8644
activation[5] = self(-0.8644) + output[4](0.4440) * 1.0000 => -0.4203
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5729) * 1.0000 => 0.5729
activation[8] = self(0.0000) + output[5](0.0404) * 1.0000 => 0.0404
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9461)
networkOutput[2] := neuronOutput[8](0.5503)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=022 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0683) * weight[3,3](0.0100) => 0.0007
activation[4] = self(0.2414) * weight[4,4](0.7000) => 0.1690
activation[5] = self(-0.4203) * weight[5,5](0.1000) => -0.0420
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5729) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0404) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0007) + output[1](0.5000) * 1.0000 => 0.5007
activation[3] = self(0.5007) + output[2](0.5000) * -0.8000 => 0.1007
activation[3] = self(0.1007) + output[5](0.1089) * -0.8000 => 0.0135
activation[4] = self(0.1690) + output[2](0.5000) * -0.6000 => -0.1310
activation[4] = self(-0.1310) + output[3](0.5845) * 1.0000 => 0.4535
activation[5] = self(-0.0420) + output[2](0.5000) * -0.8000 => -0.4420
activation[5] = self(-0.4420) + output[3](0.5845) * -0.7000 => -0.8512
activation[5] = self(-0.8512) + output[4](0.7698) * 1.0000 => -0.0814
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5845) * 1.0000 => 0.5845
activation[8] = self(0.0000) + output[5](0.1089) * 1.0000 => 0.1089
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9489)
networkOutput[2] := neuronOutput[8](0.6329)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=023 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0135) * weight[3,3](0.0100) => 0.0001
activation[4] = self(0.4535) * weight[4,4](0.7000) => 0.3175
activation[5] = self(-0.0814) * weight[5,5](0.1000) => -0.0081
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5845) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.1089) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0001) + output[1](0.5000) * 1.0000 => 0.5001
activation[3] = self(0.5001) + output[2](0.5000) * -0.8000 => 0.1001
activation[3] = self(0.1001) + output[5](0.3996) * -0.8000 => -0.2196
activation[4] = self(0.3175) + output[2](0.5000) * -0.6000 => 0.0175
activation[4] = self(0.0175) + output[3](0.5169) * 1.0000 => 0.5344
activation[5] = self(-0.0081) + output[2](0.5000) * -0.8000 => -0.4081
activation[5] = self(-0.4081) + output[3](0.5169) * -0.7000 => -0.7700
activation[5] = self(-0.7700) + output[4](0.9062) * 1.0000 => 0.1362
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5169) * 1.0000 => 0.5169
activation[8] = self(0.0000) + output[5](0.3996) * 1.0000 => 0.3996
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9299)
networkOutput[2] := neuronOutput[8](0.8806)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=024 --
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.2196) * weight[3,3](0.0100) => -0.0022
activation[4] = self(0.5344) * weight[4,4](0.7000) => 0.3741
activation[5] = self(0.1362) * weight[5,5](0.1000) => 0.0136
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5169) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3996) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0022) + output[1](0.9933) * 1.0000 => 0.9911
activation[3] = self(0.9911) + output[2](0.5000) * -0.8000 => 0.5911
activation[3] = self(0.5911) + output[5](0.6639) * -0.8000 => 0.0600
activation[4] = self(0.3741) + output[2](0.5000) * -0.6000 => 0.0741
activation[4] = self(0.0741) + output[3](0.2501) * 1.0000 => 0.3242
activation[5] = self(0.0136) + output[2](0.5000) * -0.8000 => -0.3864
activation[5] = self(-0.3864) + output[3](0.2501) * -0.7000 => -0.5615
activation[5] = self(-0.5615) + output[4](0.9353) * 1.0000 => 0.3739
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.2501) * 1.0000 => 0.2501
activation[8] = self(0.0000) + output[5](0.6639) * 1.0000 => 0.6639
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.7774)
networkOutput[2] := neuronOutput[8](0.9651)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=025 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0600) * weight[3,3](0.0100) => 0.0006
activation[4] = self(0.3242) * weight[4,4](0.7000) => 0.2269
activation[5] = self(0.3739) * weight[5,5](0.1000) => 0.0374
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.2501) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6639) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0006) + output[1](0.5000) * 1.0000 => 0.5006
activation[3] = self(0.5006) + output[2](0.5000) * -0.8000 => 0.1006
activation[3] = self(0.1006) + output[5](0.8664) * -0.8000 => -0.5925
activation[4] = self(0.2269) + output[2](0.5000) * -0.6000 => -0.0731
activation[4] = self(-0.0731) + output[3](0.5744) * 1.0000 => 0.5013
activation[5] = self(0.0374) + output[2](0.5000) * -0.8000 => -0.3626
activation[5] = self(-0.3626) + output[3](0.5744) * -0.7000 => -0.7647
activation[5] = self(-0.7647) + output[4](0.8349) * 1.0000 => 0.0702
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5744) * 1.0000 => 0.5744
activation[8] = self(0.0000) + output[5](0.8664) * 1.0000 => 0.8664
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9464)
networkOutput[2] := neuronOutput[8](0.9870)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=026 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.5925) * weight[3,3](0.0100) => -0.0059
activation[4] = self(0.5013) * weight[4,4](0.7000) => 0.3509
activation[5] = self(0.0702) * weight[5,5](0.1000) => 0.0070
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5744) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.8664) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0059) + output[1](0.5000) * 1.0000 => 0.4941
activation[3] = self(0.4941) + output[2](0.5000) * -0.8000 => 0.0941
activation[3] = self(0.0941) + output[5](0.5869) * -0.8000 => -0.3754
activation[4] = self(0.3509) + output[2](0.5000) * -0.6000 => 0.0509
activation[4] = self(0.0509) + output[3](0.0491) * 1.0000 => 0.1001
activation[5] = self(0.0070) + output[2](0.5000) * -0.8000 => -0.3930
activation[5] = self(-0.3930) + output[3](0.0491) * -0.7000 => -0.4274
activation[5] = self(-0.4274) + output[4](0.9246) * 1.0000 => 0.4972
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0491) * 1.0000 => 0.0491
activation[8] = self(0.0000) + output[5](0.5869) * 1.0000 => 0.5869
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5611)
networkOutput[2] := neuronOutput[8](0.9495)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=027 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.3754) * weight[3,3](0.0100) => -0.0038
activation[4] = self(0.1001) * weight[4,4](0.7000) => 0.0701
activation[5] = self(0.4972) * weight[5,5](0.1000) => 0.0497
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0491) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.5869) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0038) + output[1](0.5000) * 1.0000 => 0.4962
activation[3] = self(0.4962) + output[2](0.5000) * -0.8000 => 0.0962
activation[3] = self(0.0962) + output[5](0.9232) * -0.8000 => -0.6423
activation[4] = self(0.0701) + output[2](0.5000) * -0.6000 => -0.2299
activation[4] = self(-0.2299) + output[3](0.1327) * 1.0000 => -0.0972
activation[5] = self(0.0497) + output[2](0.5000) * -0.8000 => -0.3503
activation[5] = self(-0.3503) + output[3](0.1327) * -0.7000 => -0.4432
activation[5] = self(-0.4432) + output[4](0.6226) * 1.0000 => 0.1794
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.1327) * 1.0000 => 0.1327
activation[8] = self(0.0000) + output[5](0.9232) * 1.0000 => 0.9232
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.6601)
networkOutput[2] := neuronOutput[8](0.9902)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=028 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.6423) * weight[3,3](0.0100) => -0.0064
activation[4] = self(-0.0972) * weight[4,4](0.7000) => -0.0681
activation[5] = self(0.1794) * weight[5,5](0.1000) => 0.0179
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.1327) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.9232) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0064) + output[1](0.5000) * 1.0000 => 0.4936
activation[3] = self(0.4936) + output[2](0.5000) * -0.8000 => 0.0936
activation[3] = self(0.0936) + output[5](0.7103) * -0.8000 => -0.4747
activation[4] = self(-0.0681) + output[2](0.5000) * -0.6000 => -0.3681
activation[4] = self(-0.3681) + output[3](0.0387) * 1.0000 => -0.3293
activation[5] = self(0.0179) + output[2](0.5000) * -0.8000 => -0.3821
activation[5] = self(-0.3821) + output[3](0.0387) * -0.7000 => -0.4092
activation[5] = self(-0.4092) + output[4](0.3808) * 1.0000 => -0.0284
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0387) * 1.0000 => 0.0387
activation[8] = self(0.0000) + output[5](0.7103) * 1.0000 => 0.7103
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5483)
networkOutput[2] := neuronOutput[8](0.9721)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=029 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.4747) * weight[3,3](0.0100) => -0.0047
activation[4] = self(-0.3293) * weight[4,4](0.7000) => -0.2305
activation[5] = self(-0.0284) * weight[5,5](0.1000) => -0.0028
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0387) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7103) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0047) + output[1](0.5000) * 1.0000 => 0.4953
activation[3] = self(0.4953) + output[2](0.5000) * -0.8000 => 0.0953
activation[3] = self(0.0953) + output[5](0.4646) * -0.8000 => -0.2764
activation[4] = self(-0.2305) + output[2](0.5000) * -0.6000 => -0.5305
activation[4] = self(-0.5305) + output[3](0.0852) * 1.0000 => -0.4453
activation[5] = self(-0.0028) + output[2](0.5000) * -0.8000 => -0.4028
activation[5] = self(-0.4028) + output[3](0.0852) * -0.7000 => -0.4625
activation[5] = self(-0.4625) + output[4](0.1616) * 1.0000 => -0.3009
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0852) * 1.0000 => 0.0852
activation[8] = self(0.0000) + output[5](0.4646) * 1.0000 => 0.4646
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.6049)
networkOutput[2] := neuronOutput[8](0.9108)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=030 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.2764) * weight[3,3](0.0100) => -0.0028
activation[4] = self(-0.4453) * weight[4,4](0.7000) => -0.3117
activation[5] = self(-0.3009) * weight[5,5](0.1000) => -0.0301
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0852) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4646) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0028) + output[1](0.5000) * 1.0000 => 0.4972
activation[3] = self(0.4972) + output[2](0.5000) * -0.8000 => 0.0972
activation[3] = self(0.0972) + output[5](0.1817) * -0.8000 => -0.0481
activation[4] = self(-0.3117) + output[2](0.5000) * -0.6000 => -0.6117
activation[4] = self(-0.6117) + output[3](0.2007) * 1.0000 => -0.4110
activation[5] = self(-0.0301) + output[2](0.5000) * -0.8000 => -0.4301
activation[5] = self(-0.4301) + output[3](0.2007) * -0.7000 => -0.5706
activation[5] = self(-0.5706) + output[4](0.0974) * 1.0000 => -0.4732
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.2007) * 1.0000 => 0.2007
activation[8] = self(0.0000) + output[5](0.1817) * 1.0000 => 0.1817
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.7317)
networkOutput[2] := neuronOutput[8](0.7127)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=031 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.0481) * weight[3,3](0.0100) => -0.0005
activation[4] = self(-0.4110) * weight[4,4](0.7000) => -0.2877
activation[5] = self(-0.4732) * weight[5,5](0.1000) => -0.0473
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.2007) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.1817) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0005) + output[1](0.5000) * 1.0000 => 0.4995
activation[3] = self(0.4995) + output[2](0.5000) * -0.8000 => 0.0995
activation[3] = self(0.0995) + output[5](0.0858) * -0.8000 => 0.0309
activation[4] = self(-0.2877) + output[2](0.5000) * -0.6000 => -0.5877
activation[4] = self(-0.5877) + output[3](0.4401) * 1.0000 => -0.1476
activation[5] = self(-0.0473) + output[2](0.5000) * -0.8000 => -0.4473
activation[5] = self(-0.4473) + output[3](0.4401) * -0.7000 => -0.7554
activation[5] = self(-0.7554) + output[4](0.1135) * 1.0000 => -0.6419
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.4401) * 1.0000 => 0.4401
activation[8] = self(0.0000) + output[5](0.0858) * 1.0000 => 0.0858
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9003)
networkOutput[2] := neuronOutput[8](0.6056)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=032 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0309) * weight[3,3](0.0100) => 0.0003
activation[4] = self(-0.1476) * weight[4,4](0.7000) => -0.1033
activation[5] = self(-0.6419) * weight[5,5](0.1000) => -0.0642
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.4401) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0858) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0003) + output[1](0.5000) * 1.0000 => 0.5003
activation[3] = self(0.5003) + output[2](0.5000) * -0.8000 => 0.1003
activation[3] = self(0.1003) + output[5](0.0388) * -0.8000 => 0.0693
activation[4] = self(-0.1033) + output[2](0.5000) * -0.6000 => -0.4033
activation[4] = self(-0.4033) + output[3](0.5385) * 1.0000 => 0.1352
activation[5] = self(-0.0642) + output[2](0.5000) * -0.8000 => -0.4642
activation[5] = self(-0.4642) + output[3](0.5385) * -0.7000 => -0.8411
activation[5] = self(-0.8411) + output[4](0.3234) * 1.0000 => -0.5177
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5385) * 1.0000 => 0.5385
activation[8] = self(0.0000) + output[5](0.0388) * 1.0000 => 0.0388
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9366)
networkOutput[2] := neuronOutput[8](0.5484)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=033 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0693) * weight[3,3](0.0100) => 0.0007
activation[4] = self(0.1352) * weight[4,4](0.7000) => 0.0946
activation[5] = self(-0.5177) * weight[5,5](0.1000) => -0.0518
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5385) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0388) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0007) + output[1](0.5000) * 1.0000 => 0.5007
activation[3] = self(0.5007) + output[2](0.5000) * -0.8000 => 0.1007
activation[3] = self(0.1007) + output[5](0.0699) * -0.8000 => 0.0448
activation[4] = self(0.0946) + output[2](0.5000) * -0.6000 => -0.2054
activation[4] = self(-0.2054) + output[3](0.5857) * 1.0000 => 0.3803
activation[5] = self(-0.0518) + output[2](0.5000) * -0.8000 => -0.4518
activation[5] = self(-0.4518) + output[3](0.5857) * -0.7000 => -0.8618
activation[5] = self(-0.8618) + output[4](0.6628) * 1.0000 => -0.1990
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5857) * 1.0000 => 0.5857
activation[8] = self(0.0000) + output[5](0.0699) * 1.0000 => 0.0699
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9492)
networkOutput[2] := neuronOutput[8](0.5865)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=034 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0448) * weight[3,3](0.0100) => 0.0004
activation[4] = self(0.3803) * weight[4,4](0.7000) => 0.2662
activation[5] = self(-0.1990) * weight[5,5](0.1000) => -0.0199
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5857) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0699) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0004) + output[1](0.5000) * 1.0000 => 0.5004
activation[3] = self(0.5004) + output[2](0.5000) * -0.8000 => 0.1004
activation[3] = self(0.1004) + output[5](0.2700) * -0.8000 => -0.1155
activation[4] = self(0.2662) + output[2](0.5000) * -0.6000 => -0.0338
activation[4] = self(-0.0338) + output[3](0.5558) * 1.0000 => 0.5220
activation[5] = self(-0.0199) + output[2](0.5000) * -0.8000 => -0.4199
activation[5] = self(-0.4199) + output[3](0.5558) * -0.7000 => -0.8089
activation[5] = self(-0.8089) + output[4](0.8701) * 1.0000 => 0.0612
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5558) * 1.0000 => 0.5558
activation[8] = self(0.0000) + output[5](0.2700) * 1.0000 => 0.2700
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9415)
networkOutput[2] := neuronOutput[8](0.7941)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=035 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.1155) * weight[3,3](0.0100) => -0.0012
activation[4] = self(0.5220) * weight[4,4](0.7000) => 0.3654
activation[5] = self(0.0612) * weight[5,5](0.1000) => 0.0061
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5558) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.2700) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0012) + output[1](0.5000) * 1.0000 => 0.4988
activation[3] = self(0.4988) + output[2](0.5000) * -0.8000 => 0.0988
activation[3] = self(0.0988) + output[5](0.5759) * -0.8000 => -0.3618
activation[4] = self(0.3654) + output[2](0.5000) * -0.6000 => 0.0654
activation[4] = self(0.0654) + output[3](0.3595) * 1.0000 => 0.4249
activation[5] = self(0.0061) + output[2](0.5000) * -0.8000 => -0.3939
activation[5] = self(-0.3939) + output[3](0.3595) * -0.7000 => -0.6455
activation[5] = self(-0.6455) + output[4](0.9315) * 1.0000 => 0.2860
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.3595) * 1.0000 => 0.3595
activation[8] = self(0.0000) + output[5](0.5759) * 1.0000 => 0.5759
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.8578)
networkOutput[2] := neuronOutput[8](0.9468)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=036 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.3618) * weight[3,3](0.0100) => -0.0036
activation[4] = self(0.4249) * weight[4,4](0.7000) => 0.2974
activation[5] = self(0.2860) * weight[5,5](0.1000) => 0.0286
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.3595) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.5759) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0036) + output[1](0.5000) * 1.0000 => 0.4964
activation[3] = self(0.4964) + output[2](0.5000) * -0.8000 => 0.0964
activation[3] = self(0.0964) + output[5](0.8069) * -0.8000 => -0.5491
activation[4] = self(0.2974) + output[2](0.5000) * -0.6000 => -0.0026
activation[4] = self(-0.0026) + output[3](0.1407) * 1.0000 => 0.1381
activation[5] = self(0.0286) + output[2](0.5000) * -0.8000 => -0.3714
activation[5] = self(-0.3714) + output[3](0.1407) * -0.7000 => -0.4699
activation[5] = self(-0.4699) + output[4](0.8932) * 1.0000 => 0.4233
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.1407) * 1.0000 => 0.1407
activation[8] = self(0.0000) + output[5](0.8069) * 1.0000 => 0.8069
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.6690)
networkOutput[2] := neuronOutput[8](0.9826)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=037 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.5491) * weight[3,3](0.0100) => -0.0055
activation[4] = self(0.1381) * weight[4,4](0.7000) => 0.0967
activation[5] = self(0.4233) * weight[5,5](0.1000) => 0.0423
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.1407) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.8069) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0055) + output[1](0.5000) * 1.0000 => 0.4945
activation[3] = self(0.4945) + output[2](0.5000) * -0.8000 => 0.0945
activation[3] = self(0.0945) + output[5](0.8925) * -0.8000 => -0.6195
activation[4] = self(0.0967) + output[2](0.5000) * -0.6000 => -0.2033
activation[4] = self(-0.2033) + output[3](0.0603) * 1.0000 => -0.1430
activation[5] = self(0.0423) + output[2](0.5000) * -0.8000 => -0.3577
activation[5] = self(-0.3577) + output[3](0.0603) * -0.7000 => -0.3999
activation[5] = self(-0.3999) + output[4](0.6661) * 1.0000 => 0.2662
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0603) * 1.0000 => 0.0603
activation[8] = self(0.0000) + output[5](0.8925) * 1.0000 => 0.8925
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5748)
networkOutput[2] := neuronOutput[8](0.9886)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=038 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.6195) * weight[3,3](0.0100) => -0.0062
activation[4] = self(-0.1430) * weight[4,4](0.7000) => -0.1001
activation[5] = self(0.2662) * weight[5,5](0.1000) => 0.0266
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0603) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.8925) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0062) + output[1](0.5000) * 1.0000 => 0.4938
activation[3] = self(0.4938) + output[2](0.5000) * -0.8000 => 0.0938
activation[3] = self(0.0938) + output[5](0.7910) * -0.8000 => -0.5390
activation[4] = self(-0.1001) + output[2](0.5000) * -0.6000 => -0.4001
activation[4] = self(-0.4001) + output[3](0.0432) * 1.0000 => -0.3569
activation[5] = self(0.0266) + output[2](0.5000) * -0.8000 => -0.3734
activation[5] = self(-0.3734) + output[3](0.0432) * -0.7000 => -0.4036
activation[5] = self(-0.4036) + output[4](0.3285) * 1.0000 => -0.0751
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0432) * 1.0000 => 0.0432
activation[8] = self(0.0000) + output[5](0.7910) * 1.0000 => 0.7910
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5538)
networkOutput[2] := neuronOutput[8](0.9812)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=039 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.5390) * weight[3,3](0.0100) => -0.0054
activation[4] = self(-0.3569) * weight[4,4](0.7000) => -0.2498
activation[5] = self(-0.0751) * weight[5,5](0.1000) => -0.0075
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0432) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7910) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0054) + output[1](0.5000) * 1.0000 => 0.4946
activation[3] = self(0.4946) + output[2](0.5000) * -0.8000 => 0.0946
activation[3] = self(0.0946) + output[5](0.4072) * -0.8000 => -0.2312
activation[4] = self(-0.2498) + output[2](0.5000) * -0.6000 => -0.5498
activation[4] = self(-0.5498) + output[3](0.0633) * 1.0000 => -0.4865
activation[5] = self(-0.0075) + output[2](0.5000) * -0.8000 => -0.4075
activation[5] = self(-0.4075) + output[3](0.0633) * -0.7000 => -0.4518
activation[5] = self(-0.4518) + output[4](0.1438) * 1.0000 => -0.3080
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0633) * 1.0000 => 0.0633
activation[8] = self(0.0000) + output[5](0.4072) * 1.0000 => 0.4072
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5784)
networkOutput[2] := neuronOutput[8](0.8845)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=040 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.2312) * weight[3,3](0.0100) => -0.0023
activation[4] = self(-0.4865) * weight[4,4](0.7000) => -0.3406
activation[5] = self(-0.3080) * weight[5,5](0.1000) => -0.0308
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0633) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4072) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0023) + output[1](0.5000) * 1.0000 => 0.4977
activation[3] = self(0.4977) + output[2](0.5000) * -0.8000 => 0.0977
activation[3] = self(0.0977) + output[5](0.1765) * -0.8000 => -0.0435
activation[4] = self(-0.3406) + output[2](0.5000) * -0.6000 => -0.6406
activation[4] = self(-0.6406) + output[3](0.2394) * 1.0000 => -0.4012
activation[5] = self(-0.0308) + output[2](0.5000) * -0.8000 => -0.4308
activation[5] = self(-0.4308) + output[3](0.2394) * -0.7000 => -0.5984
activation[5] = self(-0.5984) + output[4](0.0807) * 1.0000 => -0.5177
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.2394) * 1.0000 => 0.2394
activation[8] = self(0.0000) + output[5](0.1765) * 1.0000 => 0.1765
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.7680)
networkOutput[2] := neuronOutput[8](0.7074)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=041 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.0435) * weight[3,3](0.0100) => -0.0004
activation[4] = self(-0.4012) * weight[4,4](0.7000) => -0.2808
activation[5] = self(-0.5177) * weight[5,5](0.1000) => -0.0518
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.2394) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.1765) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0004) + output[1](0.5000) * 1.0000 => 0.4996
activation[3] = self(0.4996) + output[2](0.5000) * -0.8000 => 0.0996
activation[3] = self(0.0996) + output[5](0.0699) * -0.8000 => 0.0437
activation[4] = self(-0.2808) + output[2](0.5000) * -0.6000 => -0.5808
activation[4] = self(-0.5808) + output[3](0.4458) * 1.0000 => -0.1350
activation[5] = self(-0.0518) + output[2](0.5000) * -0.8000 => -0.4518
activation[5] = self(-0.4518) + output[3](0.4458) * -0.7000 => -0.7638
activation[5] = self(-0.7638) + output[4](0.1186) * 1.0000 => -0.6452
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.4458) * 1.0000 => 0.4458
activation[8] = self(0.0000) + output[5](0.0699) * 1.0000 => 0.0699
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9028)
networkOutput[2] := neuronOutput[8](0.5865)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=042 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0437) * weight[3,3](0.0100) => 0.0004
activation[4] = self(-0.1350) * weight[4,4](0.7000) => -0.0945
activation[5] = self(-0.6452) * weight[5,5](0.1000) => -0.0645
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.4458) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0699) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0004) + output[1](0.5000) * 1.0000 => 0.5004
activation[3] = self(0.5004) + output[2](0.5000) * -0.8000 => 0.1004
activation[3] = self(0.1004) + output[5](0.0382) * -0.8000 => 0.0699
activation[4] = self(-0.0945) + output[2](0.5000) * -0.6000 => -0.3945
activation[4] = self(-0.3945) + output[3](0.5544) * 1.0000 => 0.1599
activation[5] = self(-0.0645) + output[2](0.5000) * -0.8000 => -0.4645
activation[5] = self(-0.4645) + output[3](0.5544) * -0.7000 => -0.8526
activation[5] = self(-0.8526) + output[4](0.3374) * 1.0000 => -0.5152
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5544) * 1.0000 => 0.5544
activation[8] = self(0.0000) + output[5](0.0382) * 1.0000 => 0.0382
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9411)
networkOutput[2] := neuronOutput[8](0.5476)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=043 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0699) * weight[3,3](0.0100) => 0.0007
activation[4] = self(0.1599) * weight[4,4](0.7000) => 0.1119
activation[5] = self(-0.5152) * weight[5,5](0.1000) => -0.0515
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5544) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0382) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0007) + output[1](0.5000) * 1.0000 => 0.5007
activation[3] = self(0.5007) + output[2](0.5000) * -0.8000 => 0.1007
activation[3] = self(0.1007) + output[5](0.0707) * -0.8000 => 0.0441
activation[4] = self(0.1119) + output[2](0.5000) * -0.6000 => -0.1881
activation[4] = self(-0.1881) + output[3](0.5865) * 1.0000 => 0.3984
activation[5] = self(-0.0515) + output[2](0.5000) * -0.8000 => -0.4515
activation[5] = self(-0.4515) + output[3](0.5865) * -0.7000 => -0.8621
activation[5] = self(-0.8621) + output[4](0.6898) * 1.0000 => -0.1722
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5865) * 1.0000 => 0.5865
activation[8] = self(0.0000) + output[5](0.0707) * 1.0000 => 0.0707
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9494)
networkOutput[2] := neuronOutput[8](0.5875)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=044 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0441) * weight[3,3](0.0100) => 0.0004
activation[4] = self(0.3984) * weight[4,4](0.7000) => 0.2789
activation[5] = self(-0.1722) * weight[5,5](0.1000) => -0.0172
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5865) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0707) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0004) + output[1](0.5000) * 1.0000 => 0.5004
activation[3] = self(0.5004) + output[2](0.5000) * -0.8000 => 0.1004
activation[3] = self(0.1004) + output[5](0.2971) * -0.8000 => -0.1372
activation[4] = self(0.2789) + output[2](0.5000) * -0.6000 => -0.0211
activation[4] = self(-0.0211) + output[3](0.5549) * 1.0000 => 0.5338
activation[5] = self(-0.0172) + output[2](0.5000) * -0.8000 => -0.4172
activation[5] = self(-0.4172) + output[3](0.5549) * -0.7000 => -0.8057
activation[5] = self(-0.8057) + output[4](0.8799) * 1.0000 => 0.0743
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5549) * 1.0000 => 0.5549
activation[8] = self(0.0000) + output[5](0.2971) * 1.0000 => 0.2971
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9413)
networkOutput[2] := neuronOutput[8](0.8154)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=045 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.1372) * weight[3,3](0.0100) => -0.0014
activation[4] = self(0.5338) * weight[4,4](0.7000) => 0.3737
activation[5] = self(0.0743) * weight[5,5](0.1000) => 0.0074
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5549) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.2971) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0014) + output[1](0.5000) * 1.0000 => 0.4986
activation[3] = self(0.4986) + output[2](0.5000) * -0.8000 => 0.0986
activation[3] = self(0.0986) + output[5](0.5918) * -0.8000 => -0.3748
activation[4] = self(0.3737) + output[2](0.5000) * -0.6000 => 0.0737
activation[4] = self(0.0737) + output[3](0.3349) * 1.0000 => 0.4086
activation[5] = self(0.0074) + output[2](0.5000) * -0.8000 => -0.3926
activation[5] = self(-0.3926) + output[3](0.3349) * -0.7000 => -0.6270
activation[5] = self(-0.6270) + output[4](0.9352) * 1.0000 => 0.3082
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.3349) * 1.0000 => 0.3349
activation[8] = self(0.0000) + output[5](0.5918) * 1.0000 => 0.5918
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.8422)
networkOutput[2] := neuronOutput[8](0.9507)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=046 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.3748) * weight[3,3](0.0100) => -0.0037
activation[4] = self(0.4086) * weight[4,4](0.7000) => 0.2860
activation[5] = self(0.3082) * weight[5,5](0.1000) => 0.0308
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.3349) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.5918) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0037) + output[1](0.5000) * 1.0000 => 0.4963
activation[3] = self(0.4963) + output[2](0.5000) * -0.8000 => 0.0963
activation[3] = self(0.0963) + output[5](0.8236) * -0.8000 => -0.5626
activation[4] = self(0.2860) + output[2](0.5000) * -0.6000 => -0.0140
activation[4] = self(-0.0140) + output[3](0.1331) * 1.0000 => 0.1191
activation[5] = self(0.0308) + output[2](0.5000) * -0.8000 => -0.3692
activation[5] = self(-0.3692) + output[3](0.1331) * -0.7000 => -0.4623
activation[5] = self(-0.4623) + output[4](0.8852) * 1.0000 => 0.4229
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.1331) * 1.0000 => 0.1331
activation[8] = self(0.0000) + output[5](0.8236) * 1.0000 => 0.8236
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.6605)
networkOutput[2] := neuronOutput[8](0.9840)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=047 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.5626) * weight[3,3](0.0100) => -0.0056
activation[4] = self(0.1191) * weight[4,4](0.7000) => 0.0834
activation[5] = self(0.4229) * weight[5,5](0.1000) => 0.0423
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.1331) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.8236) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0056) + output[1](0.5000) * 1.0000 => 0.4944
activation[3] = self(0.4944) + output[2](0.5000) * -0.8000 => 0.0944
activation[3] = self(0.0944) + output[5](0.8923) * -0.8000 => -0.6195
activation[4] = self(0.0834) + output[2](0.5000) * -0.6000 => -0.2166
activation[4] = self(-0.2166) + output[3](0.0566) * 1.0000 => -0.1600
activation[5] = self(0.0423) + output[2](0.5000) * -0.8000 => -0.3577
activation[5] = self(-0.3577) + output[3](0.0566) * -0.7000 => -0.3973
activation[5] = self(-0.3973) + output[4](0.6446) * 1.0000 => 0.2473
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0566) * 1.0000 => 0.0566
activation[8] = self(0.0000) + output[5](0.8923) * 1.0000 => 0.8923
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5703)
networkOutput[2] := neuronOutput[8](0.9886)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=048 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.6195) * weight[3,3](0.0100) => -0.0062
activation[4] = self(-0.1600) * weight[4,4](0.7000) => -0.1120
activation[5] = self(0.2473) * weight[5,5](0.1000) => 0.0247
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0566) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.8923) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0062) + output[1](0.5000) * 1.0000 => 0.4938
activation[3] = self(0.4938) + output[2](0.5000) * -0.8000 => 0.0938
activation[3] = self(0.0938) + output[5](0.7749) * -0.8000 => -0.5261
activation[4] = self(-0.1120) + output[2](0.5000) * -0.6000 => -0.4120
activation[4] = self(-0.4120) + output[3](0.0432) * 1.0000 => -0.3688
activation[5] = self(0.0247) + output[2](0.5000) * -0.8000 => -0.3753
activation[5] = self(-0.3753) + output[3](0.0432) * -0.7000 => -0.4055
activation[5] = self(-0.4055) + output[4](0.3100) * 1.0000 => -0.0955
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0432) * 1.0000 => 0.0432
activation[8] = self(0.0000) + output[5](0.7749) * 1.0000 => 0.7749
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5538)
networkOutput[2] := neuronOutput[8](0.9797)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=049 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.5261) * weight[3,3](0.0100) => -0.0053
activation[4] = self(-0.3688) * weight[4,4](0.7000) => -0.2582
activation[5] = self(-0.0955) * weight[5,5](0.1000) => -0.0096
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0432) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7749) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0053) + output[1](0.5000) * 1.0000 => 0.4947
activation[3] = self(0.4947) + output[2](0.5000) * -0.8000 => 0.0947
activation[3] = self(0.0947) + output[5](0.3828) * -0.8000 => -0.2115
activation[4] = self(-0.2582) + output[2](0.5000) * -0.6000 => -0.5582
activation[4] = self(-0.5582) + output[3](0.0672) * 1.0000 => -0.4910
activation[5] = self(-0.0096) + output[2](0.5000) * -0.8000 => -0.4096
activation[5] = self(-0.4096) + output[3](0.0672) * -0.7000 => -0.4566
activation[5] = self(-0.4566) + output[4](0.1366) * 1.0000 => -0.3200
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0672) * 1.0000 => 0.0672
activation[8] = self(0.0000) + output[5](0.3828) * 1.0000 => 0.3828
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5832)
networkOutput[2] := neuronOutput[8](0.8715)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=050 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.2115) * weight[3,3](0.0100) => -0.0021
activation[4] = self(-0.4910) * weight[4,4](0.7000) => -0.3437
activation[5] = self(-0.3200) * weight[5,5](0.1000) => -0.0320
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0672) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3828) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0021) + output[1](0.5000) * 1.0000 => 0.4979
activation[3] = self(0.4979) + output[2](0.5000) * -0.8000 => 0.0979
activation[3] = self(0.0979) + output[5](0.1680) * -0.8000 => -0.0365
activation[4] = self(-0.3437) + output[2](0.5000) * -0.6000 => -0.6437
activation[4] = self(-0.6437) + output[3](0.2578) * 1.0000 => -0.3859
activation[5] = self(-0.0320) + output[2](0.5000) * -0.8000 => -0.4320
activation[5] = self(-0.4320) + output[3](0.2578) * -0.7000 => -0.6124
activation[5] = self(-0.6124) + output[4](0.0791) * 1.0000 => -0.5334
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.2578) * 1.0000 => 0.2578
activation[8] = self(0.0000) + output[5](0.1680) * 1.0000 => 0.1680
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.7840)
networkOutput[2] := neuronOutput[8](0.6984)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=051 --
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.0365) * weight[3,3](0.0100) => -0.0004
activation[4] = self(-0.3859) * weight[4,4](0.7000) => -0.2701
activation[5] = self(-0.5334) * weight[5,5](0.1000) => -0.0533
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.2578) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.1680) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0004) + output[1](0.9933) * 1.0000 => 0.9929
activation[3] = self(0.9929) + output[2](0.5000) * -0.8000 => 0.5929
activation[3] = self(0.5929) + output[5](0.0650) * -0.8000 => 0.5410
activation[4] = self(-0.2701) + output[2](0.5000) * -0.6000 => -0.5701
activation[4] = self(-0.5701) + output[3](0.4545) * 1.0000 => -0.1156
activation[5] = self(-0.0533) + output[2](0.5000) * -0.8000 => -0.4533
activation[5] = self(-0.4533) + output[3](0.4545) * -0.7000 => -0.7715
activation[5] = self(-0.7715) + output[4](0.1268) * 1.0000 => -0.6447
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.4545) * 1.0000 => 0.4545
activation[8] = self(0.0000) + output[5](0.0650) * 1.0000 => 0.0650
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9066)
networkOutput[2] := neuronOutput[8](0.5805)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=052 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.5410) * weight[3,3](0.0100) => 0.0054
activation[4] = self(-0.1156) * weight[4,4](0.7000) => -0.0809
activation[5] = self(-0.6447) * weight[5,5](0.1000) => -0.0645
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.4545) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0650) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0054) + output[1](0.5000) * 1.0000 => 0.5054
activation[3] = self(0.5054) + output[2](0.5000) * -0.8000 => 0.1054
activation[3] = self(0.1054) + output[5](0.0383) * -0.8000 => 0.0748
activation[4] = self(-0.0809) + output[2](0.5000) * -0.6000 => -0.3809
activation[4] = self(-0.3809) + output[3](0.9373) * 1.0000 => 0.5564
activation[5] = self(-0.0645) + output[2](0.5000) * -0.8000 => -0.4645
activation[5] = self(-0.4645) + output[3](0.9373) * -0.7000 => -1.1206
activation[5] = self(-1.1206) + output[4](0.3594) * 1.0000 => -0.7612
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.9373) * 1.0000 => 0.9373
activation[8] = self(0.0000) + output[5](0.0383) * 1.0000 => 0.0383
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9909)
networkOutput[2] := neuronOutput[8](0.5477)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=053 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0748) * weight[3,3](0.0100) => 0.0007
activation[4] = self(0.5564) * weight[4,4](0.7000) => 0.3895
activation[5] = self(-0.7612) * weight[5,5](0.1000) => -0.0761
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.9373) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0383) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0007) + output[1](0.5000) * 1.0000 => 0.5007
activation[3] = self(0.5007) + output[2](0.5000) * -0.8000 => 0.1007
activation[3] = self(0.1007) + output[5](0.0218) * -0.8000 => 0.0833
activation[4] = self(0.3895) + output[2](0.5000) * -0.6000 => 0.0895
activation[4] = self(0.0895) + output[3](0.5924) * 1.0000 => 0.6819
activation[5] = self(-0.0761) + output[2](0.5000) * -0.8000 => -0.4761
activation[5] = self(-0.4761) + output[3](0.5924) * -0.7000 => -0.8908
activation[5] = self(-0.8908) + output[4](0.9417) * 1.0000 => 0.0509
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5924) * 1.0000 => 0.5924
activation[8] = self(0.0000) + output[5](0.0218) * 1.0000 => 0.0218
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9508)
networkOutput[2] := neuronOutput[8](0.5272)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=054 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0833) * weight[3,3](0.0100) => 0.0008
activation[4] = self(0.6819) * weight[4,4](0.7000) => 0.4773
activation[5] = self(0.0509) * weight[5,5](0.1000) => 0.0051
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5924) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0218) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0008) + output[1](0.5000) * 1.0000 => 0.5008
activation[3] = self(0.5008) + output[2](0.5000) * -0.8000 => 0.1008
activation[3] = self(0.1008) + output[5](0.5633) * -0.8000 => -0.3498
activation[4] = self(0.4773) + output[2](0.5000) * -0.6000 => 0.1773
activation[4] = self(0.1773) + output[3](0.6027) * 1.0000 => 0.7800
activation[5] = self(0.0051) + output[2](0.5000) * -0.8000 => -0.3949
activation[5] = self(-0.3949) + output[3](0.6027) * -0.7000 => -0.8168
activation[5] = self(-0.8168) + output[4](0.9680) * 1.0000 => 0.1512
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.6027) * 1.0000 => 0.6027
activation[8] = self(0.0000) + output[5](0.5633) * 1.0000 => 0.5633
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9532)
networkOutput[2] := neuronOutput[8](0.9436)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=055 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.3498) * weight[3,3](0.0100) => -0.0035
activation[4] = self(0.7800) * weight[4,4](0.7000) => 0.5460
activation[5] = self(0.1512) * weight[5,5](0.1000) => 0.0151
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.6027) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.5633) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0035) + output[1](0.5000) * 1.0000 => 0.4965
activation[3] = self(0.4965) + output[2](0.5000) * -0.8000 => 0.0965
activation[3] = self(0.0965) + output[5](0.6805) * -0.8000 => -0.4479
activation[4] = self(0.5460) + output[2](0.5000) * -0.6000 => 0.2460
activation[4] = self(0.2460) + output[3](0.1482) * 1.0000 => 0.3942
activation[5] = self(0.0151) + output[2](0.5000) * -0.8000 => -0.3849
activation[5] = self(-0.3849) + output[3](0.1482) * -0.7000 => -0.4886
activation[5] = self(-0.4886) + output[4](0.9802) * 1.0000 => 0.4916
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.1482) * 1.0000 => 0.1482
activation[8] = self(0.0000) + output[5](0.6805) * 1.0000 => 0.6805
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.6772)
networkOutput[2] := neuronOutput[8](0.9678)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=056 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.4479) * weight[3,3](0.0100) => -0.0045
activation[4] = self(0.3942) * weight[4,4](0.7000) => 0.2759
activation[5] = self(0.4916) * weight[5,5](0.1000) => 0.0492
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.1482) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6805) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0045) + output[1](0.5000) * 1.0000 => 0.4955
activation[3] = self(0.4955) + output[2](0.5000) * -0.8000 => 0.0955
activation[3] = self(0.0955) + output[5](0.9211) * -0.8000 => -0.6414
activation[4] = self(0.2759) + output[2](0.5000) * -0.6000 => -0.0241
activation[4] = self(-0.0241) + output[3](0.0963) * 1.0000 => 0.0722
activation[5] = self(0.0492) + output[2](0.5000) * -0.8000 => -0.3508
activation[5] = self(-0.3508) + output[3](0.0963) * -0.7000 => -0.4182
activation[5] = self(-0.4182) + output[4](0.8777) * 1.0000 => 0.4595
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0963) * 1.0000 => 0.0963
activation[8] = self(0.0000) + output[5](0.9211) * 1.0000 => 0.9211
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.6181)
networkOutput[2] := neuronOutput[8](0.9901)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=057 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.6414) * weight[3,3](0.0100) => -0.0064
activation[4] = self(0.0722) * weight[4,4](0.7000) => 0.0505
activation[5] = self(0.4595) * weight[5,5](0.1000) => 0.0459
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0963) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.9211) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0064) + output[1](0.5000) * 1.0000 => 0.4936
activation[3] = self(0.4936) + output[2](0.5000) * -0.8000 => 0.0936
activation[3] = self(0.0936) + output[5](0.9087) * -0.8000 => -0.6333
activation[4] = self(0.0505) + output[2](0.5000) * -0.6000 => -0.2495
activation[4] = self(-0.2495) + output[3](0.0389) * 1.0000 => -0.2106
activation[5] = self(0.0459) + output[2](0.5000) * -0.8000 => -0.3541
activation[5] = self(-0.3541) + output[3](0.0389) * -0.7000 => -0.3813
activation[5] = self(-0.3813) + output[4](0.5893) * 1.0000 => 0.2080
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0389) * 1.0000 => 0.0389
activation[8] = self(0.0000) + output[5](0.9087) * 1.0000 => 0.9087
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5485)
networkOutput[2] := neuronOutput[8](0.9895)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=058 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.6333) * weight[3,3](0.0100) => -0.0063
activation[4] = self(-0.2106) * weight[4,4](0.7000) => -0.1474
activation[5] = self(0.2080) * weight[5,5](0.1000) => 0.0208
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0389) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.9087) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0063) + output[1](0.5000) * 1.0000 => 0.4937
activation[3] = self(0.4937) + output[2](0.5000) * -0.8000 => 0.0937
activation[3] = self(0.0937) + output[5](0.7388) * -0.8000 => -0.4974
activation[4] = self(-0.1474) + output[2](0.5000) * -0.6000 => -0.4474
activation[4] = self(-0.4474) + output[3](0.0404) * 1.0000 => -0.4070
activation[5] = self(0.0208) + output[2](0.5000) * -0.8000 => -0.3792
activation[5] = self(-0.3792) + output[3](0.0404) * -0.7000 => -0.4075
activation[5] = self(-0.4075) + output[4](0.2587) * 1.0000 => -0.1488
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0404) * 1.0000 => 0.0404
activation[8] = self(0.0000) + output[5](0.7388) * 1.0000 => 0.7388
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5504)
networkOutput[2] := neuronOutput[8](0.9757)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=059 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.4974) * weight[3,3](0.0100) => -0.0050
activation[4] = self(-0.4070) * weight[4,4](0.7000) => -0.2849
activation[5] = self(-0.1488) * weight[5,5](0.1000) => -0.0149
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0404) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7388) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0050) + output[1](0.5000) * 1.0000 => 0.4950
activation[3] = self(0.4950) + output[2](0.5000) * -0.8000 => 0.0950
activation[3] = self(0.0950) + output[5](0.3221) * -0.8000 => -0.1627
activation[4] = self(-0.2849) + output[2](0.5000) * -0.6000 => -0.5849
activation[4] = self(-0.5849) + output[3](0.0768) * 1.0000 => -0.5081
activation[5] = self(-0.0149) + output[2](0.5000) * -0.8000 => -0.4149
activation[5] = self(-0.4149) + output[3](0.0768) * -0.7000 => -0.4686
activation[5] = self(-0.4686) + output[4](0.1156) * 1.0000 => -0.3530
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0768) * 1.0000 => 0.0768
activation[8] = self(0.0000) + output[5](0.3221) * 1.0000 => 0.3221
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5948)
networkOutput[2] := neuronOutput[8](0.8335)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=060 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.1627) * weight[3,3](0.0100) => -0.0016
activation[4] = self(-0.5081) * weight[4,4](0.7000) => -0.3557
activation[5] = self(-0.3530) * weight[5,5](0.1000) => -0.0353
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0768) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3221) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0016) + output[1](0.5000) * 1.0000 => 0.4984
activation[3] = self(0.4984) + output[2](0.5000) * -0.8000 => 0.0984
activation[3] = self(0.0984) + output[5](0.1462) * -0.8000 => -0.0185
activation[4] = self(-0.3557) + output[2](0.5000) * -0.6000 => -0.6557
activation[4] = self(-0.6557) + output[3](0.3072) * 1.0000 => -0.3485
activation[5] = self(-0.0353) + output[2](0.5000) * -0.8000 => -0.4353
activation[5] = self(-0.4353) + output[3](0.3072) * -0.7000 => -0.6503
activation[5] = self(-0.6503) + output[4](0.0731) * 1.0000 => -0.5773
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.3072) * 1.0000 => 0.3072
activation[8] = self(0.0000) + output[5](0.1462) * 1.0000 => 0.1462
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.8229)
networkOutput[2] := neuronOutput[8](0.6750)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=061 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.0185) * weight[3,3](0.0100) => -0.0002
activation[4] = self(-0.3485) * weight[4,4](0.7000) => -0.2439
activation[5] = self(-0.5773) * weight[5,5](0.1000) => -0.0577
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.3072) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.1462) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0002) + output[1](0.5000) * 1.0000 => 0.4998
activation[3] = self(0.4998) + output[2](0.5000) * -0.8000 => 0.0998
activation[3] = self(0.0998) + output[5](0.0528) * -0.8000 => 0.0575
activation[4] = self(-0.2439) + output[2](0.5000) * -0.6000 => -0.5439
activation[4] = self(-0.5439) + output[3](0.4768) * 1.0000 => -0.0671
activation[5] = self(-0.0577) + output[2](0.5000) * -0.8000 => -0.4577
activation[5] = self(-0.4577) + output[3](0.4768) * -0.7000 => -0.7915
activation[5] = self(-0.7915) + output[4](0.1490) * 1.0000 => -0.6425
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.4768) * 1.0000 => 0.4768
activation[8] = self(0.0000) + output[5](0.0528) * 1.0000 => 0.0528
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9156)
networkOutput[2] := neuronOutput[8](0.5657)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=062 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0575) * weight[3,3](0.0100) => 0.0006
activation[4] = self(-0.0671) * weight[4,4](0.7000) => -0.0470
activation[5] = self(-0.6425) * weight[5,5](0.1000) => -0.0643
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.4768) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0528) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0006) + output[1](0.5000) * 1.0000 => 0.5006
activation[3] = self(0.5006) + output[2](0.5000) * -0.8000 => 0.1006
activation[3] = self(0.1006) + output[5](0.0387) * -0.8000 => 0.0696
activation[4] = self(-0.0470) + output[2](0.5000) * -0.6000 => -0.3470
activation[4] = self(-0.3470) + output[3](0.5714) * 1.0000 => 0.2245
activation[5] = self(-0.0643) + output[2](0.5000) * -0.8000 => -0.4643
activation[5] = self(-0.4643) + output[3](0.5714) * -0.7000 => -0.8643
activation[5] = self(-0.8643) + output[4](0.4169) * 1.0000 => -0.4474
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5714) * 1.0000 => 0.5714
activation[8] = self(0.0000) + output[5](0.0387) * 1.0000 => 0.0387
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9457)
networkOutput[2] := neuronOutput[8](0.5482)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=063 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0696) * weight[3,3](0.0100) => 0.0007
activation[4] = self(0.2245) * weight[4,4](0.7000) => 0.1571
activation[5] = self(-0.4474) * weight[5,5](0.1000) => -0.0447
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5714) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0387) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0007) + output[1](0.5000) * 1.0000 => 0.5007
activation[3] = self(0.5007) + output[2](0.5000) * -0.8000 => 0.1007
activation[3] = self(0.1007) + output[5](0.0965) * -0.8000 => 0.0235
activation[4] = self(0.1571) + output[2](0.5000) * -0.6000 => -0.1429
activation[4] = self(-0.1429) + output[3](0.5862) * 1.0000 => 0.4433
activation[5] = self(-0.0447) + output[2](0.5000) * -0.8000 => -0.4447
activation[5] = self(-0.4447) + output[3](0.5862) * -0.7000 => -0.8550
activation[5] = self(-0.8550) + output[4](0.7544) * 1.0000 => -0.1006
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5862) * 1.0000 => 0.5862
activation[8] = self(0.0000) + output[5](0.0965) * 1.0000 => 0.0965
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9493)
networkOutput[2] := neuronOutput[8](0.6183)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=064 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0235) * weight[3,3](0.0100) => 0.0002
activation[4] = self(0.4433) * weight[4,4](0.7000) => 0.3103
activation[5] = self(-0.1006) * weight[5,5](0.1000) => -0.0101
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5862) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0965) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0002) + output[1](0.5000) * 1.0000 => 0.5002
activation[3] = self(0.5002) + output[2](0.5000) * -0.8000 => 0.1002
activation[3] = self(0.1002) + output[5](0.3768) * -0.8000 => -0.2012
activation[4] = self(0.3103) + output[2](0.5000) * -0.6000 => 0.0103
activation[4] = self(0.0103) + output[3](0.5293) * 1.0000 => 0.5396
activation[5] = self(-0.0101) + output[2](0.5000) * -0.8000 => -0.4101
activation[5] = self(-0.4101) + output[3](0.5293) * -0.7000 => -0.7806
activation[5] = self(-0.7806) + output[4](0.9017) * 1.0000 => 0.1211
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5293) * 1.0000 => 0.5293
activation[8] = self(0.0000) + output[5](0.3768) * 1.0000 => 0.3768
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9338)
networkOutput[2] := neuronOutput[8](0.8681)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=065 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.2012) * weight[3,3](0.0100) => -0.0020
activation[4] = self(0.5396) * weight[4,4](0.7000) => 0.3777
activation[5] = self(0.1211) * weight[5,5](0.1000) => 0.0121
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5293) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3768) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0020) + output[1](0.5000) * 1.0000 => 0.4980
activation[3] = self(0.4980) + output[2](0.5000) * -0.8000 => 0.0980
activation[3] = self(0.0980) + output[5](0.6469) * -0.8000 => -0.4196
activation[4] = self(0.3777) + output[2](0.5000) * -0.6000 => 0.0777
activation[4] = self(0.0777) + output[3](0.2678) * 1.0000 => 0.3455
activation[5] = self(0.0121) + output[2](0.5000) * -0.8000 => -0.3879
activation[5] = self(-0.3879) + output[3](0.2678) * -0.7000 => -0.5753
activation[5] = self(-0.5753) + output[4](0.9369) * 1.0000 => 0.3616
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.2678) * 1.0000 => 0.2678
activation[8] = self(0.0000) + output[5](0.6469) * 1.0000 => 0.6469
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.7923)
networkOutput[2] := neuronOutput[8](0.9621)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=066 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.4196) * weight[3,3](0.0100) => -0.0042
activation[4] = self(0.3455) * weight[4,4](0.7000) => 0.2419
activation[5] = self(0.3616) * weight[5,5](0.1000) => 0.0362
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.2678) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6469) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0042) + output[1](0.5000) * 1.0000 => 0.4958
activation[3] = self(0.4958) + output[2](0.5000) * -0.8000 => 0.0958
activation[3] = self(0.0958) + output[5](0.8591) * -0.8000 => -0.5915
activation[4] = self(0.2419) + output[2](0.5000) * -0.6000 => -0.0581
activation[4] = self(-0.0581) + output[3](0.1093) * 1.0000 => 0.0512
activation[5] = self(0.0362) + output[2](0.5000) * -0.8000 => -0.3638
activation[5] = self(-0.3638) + output[3](0.1093) * -0.7000 => -0.4404
activation[5] = self(-0.4404) + output[4](0.8491) * 1.0000 => 0.4087
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.1093) * 1.0000 => 0.1093
activation[8] = self(0.0000) + output[5](0.8591) * 1.0000 => 0.8591
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.6333)
networkOutput[2] := neuronOutput[8](0.9866)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=067 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.5915) * weight[3,3](0.0100) => -0.0059
activation[4] = self(0.0512) * weight[4,4](0.7000) => 0.0358
activation[5] = self(0.4087) * weight[5,5](0.1000) => 0.0409
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.1093) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.8591) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0059) + output[1](0.5000) * 1.0000 => 0.4941
activation[3] = self(0.4941) + output[2](0.5000) * -0.8000 => 0.0941
activation[3] = self(0.0941) + output[5](0.8853) * -0.8000 => -0.6142
activation[4] = self(0.0358) + output[2](0.5000) * -0.6000 => -0.2642
activation[4] = self(-0.2642) + output[3](0.0494) * 1.0000 => -0.2148
activation[5] = self(0.0409) + output[2](0.5000) * -0.8000 => -0.3591
activation[5] = self(-0.3591) + output[3](0.0494) * -0.7000 => -0.3937
activation[5] = self(-0.3937) + output[4](0.5636) * 1.0000 => 0.1699
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0494) * 1.0000 => 0.0494
activation[8] = self(0.0000) + output[5](0.8853) * 1.0000 => 0.8853
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5614)
networkOutput[2] := neuronOutput[8](0.9882)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=068 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.6142) * weight[3,3](0.0100) => -0.0061
activation[4] = self(-0.2148) * weight[4,4](0.7000) => -0.1504
activation[5] = self(0.1699) * weight[5,5](0.1000) => 0.0170
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0494) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.8853) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0061) + output[1](0.5000) * 1.0000 => 0.4939
activation[3] = self(0.4939) + output[2](0.5000) * -0.8000 => 0.0939
activation[3] = self(0.0939) + output[5](0.7005) * -0.8000 => -0.4665
activation[4] = self(-0.1504) + output[2](0.5000) * -0.6000 => -0.4504
activation[4] = self(-0.4504) + output[3](0.0443) * 1.0000 => -0.4060
activation[5] = self(0.0170) + output[2](0.5000) * -0.8000 => -0.3830
activation[5] = self(-0.3830) + output[3](0.0443) * -0.7000 => -0.4140
activation[5] = self(-0.4140) + output[4](0.2546) * 1.0000 => -0.1594
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0443) * 1.0000 => 0.0443
activation[8] = self(0.0000) + output[5](0.7005) * 1.0000 => 0.7005
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5552)
networkOutput[2] := neuronOutput[8](0.9708)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=069 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.4665) * weight[3,3](0.0100) => -0.0047
activation[4] = self(-0.4060) * weight[4,4](0.7000) => -0.2842
activation[5] = self(-0.1594) * weight[5,5](0.1000) => -0.0159
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0443) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7005) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0047) + output[1](0.5000) * 1.0000 => 0.4953
activation[3] = self(0.4953) + output[2](0.5000) * -0.8000 => 0.0953
activation[3] = self(0.0953) + output[5](0.3107) * -0.8000 => -0.1532
activation[4] = self(-0.2842) + output[2](0.5000) * -0.6000 => -0.5842
activation[4] = self(-0.5842) + output[3](0.0885) * 1.0000 => -0.4958
activation[5] = self(-0.0159) + output[2](0.5000) * -0.8000 => -0.4159
activation[5] = self(-0.4159) + output[3](0.0885) * -0.7000 => -0.4779
activation[5] = self(-0.4779) + output[4](0.1161) * 1.0000 => -0.3618
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0885) * 1.0000 => 0.0885
activation[8] = self(0.0000) + output[5](0.3107) * 1.0000 => 0.3107
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.6088)
networkOutput[2] := neuronOutput[8](0.8254)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=070 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.1532) * weight[3,3](0.0100) => -0.0015
activation[4] = self(-0.4958) * weight[4,4](0.7000) => -0.3470
activation[5] = self(-0.3618) * weight[5,5](0.1000) => -0.0362
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0885) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3107) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0015) + output[1](0.5000) * 1.0000 => 0.4985
activation[3] = self(0.4985) + output[2](0.5000) * -0.8000 => 0.0985
activation[3] = self(0.0985) + output[5](0.1408) * -0.8000 => -0.0141
activation[4] = self(-0.3470) + output[2](0.5000) * -0.6000 => -0.6470
activation[4] = self(-0.6470) + output[3](0.3173) * 1.0000 => -0.3297
activation[5] = self(-0.0362) + output[2](0.5000) * -0.8000 => -0.4362
activation[5] = self(-0.4362) + output[3](0.3173) * -0.7000 => -0.6583
activation[5] = self(-0.6583) + output[4](0.0774) * 1.0000 => -0.5810
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.3173) * 1.0000 => 0.3173
activation[8] = self(0.0000) + output[5](0.1408) * 1.0000 => 0.1408
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.8302)
networkOutput[2] := neuronOutput[8](0.6690)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=071 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.0141) * weight[3,3](0.0100) => -0.0001
activation[4] = self(-0.3297) * weight[4,4](0.7000) => -0.2308
activation[5] = self(-0.5810) * weight[5,5](0.1000) => -0.0581
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.3173) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.1408) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0001) + output[1](0.5000) * 1.0000 => 0.4999
activation[3] = self(0.4999) + output[2](0.5000) * -0.8000 => 0.0999
activation[3] = self(0.0999) + output[5](0.0519) * -0.8000 => 0.0583
activation[4] = self(-0.2308) + output[2](0.5000) * -0.6000 => -0.5308
activation[4] = self(-0.5308) + output[3](0.4823) * 1.0000 => -0.0485
activation[5] = self(-0.0581) + output[2](0.5000) * -0.8000 => -0.4581
activation[5] = self(-0.4581) + output[3](0.4823) * -0.7000 => -0.7957
activation[5] = self(-0.7957) + output[4](0.1613) * 1.0000 => -0.6344
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.4823) * 1.0000 => 0.4823
activation[8] = self(0.0000) + output[5](0.0519) * 1.0000 => 0.0519
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9177)
networkOutput[2] := neuronOutput[8](0.5645)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=072 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0583) * weight[3,3](0.0100) => 0.0006
activation[4] = self(-0.0485) * weight[4,4](0.7000) => -0.0339
activation[5] = self(-0.6344) * weight[5,5](0.1000) => -0.0634
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.4823) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0519) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0006) + output[1](0.5000) * 1.0000 => 0.5006
activation[3] = self(0.5006) + output[2](0.5000) * -0.8000 => 0.1006
activation[3] = self(0.1006) + output[5](0.0402) * -0.8000 => 0.0684
activation[4] = self(-0.0339) + output[2](0.5000) * -0.6000 => -0.3339
activation[4] = self(-0.3339) + output[3](0.5724) * 1.0000 => 0.2385
activation[5] = self(-0.0634) + output[2](0.5000) * -0.8000 => -0.4634
activation[5] = self(-0.4634) + output[3](0.5724) * -0.7000 => -0.8641
activation[5] = self(-0.8641) + output[4](0.4397) * 1.0000 => -0.4244
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5724) * 1.0000 => 0.5724
activation[8] = self(0.0000) + output[5](0.0402) * 1.0000 => 0.0402
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9459)
networkOutput[2] := neuronOutput[8](0.5501)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=073 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0684) * weight[3,3](0.0100) => 0.0007
activation[4] = self(0.2385) * weight[4,4](0.7000) => 0.1669
activation[5] = self(-0.4244) * weight[5,5](0.1000) => -0.0424
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5724) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0402) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0007) + output[1](0.5000) * 1.0000 => 0.5007
activation[3] = self(0.5007) + output[2](0.5000) * -0.8000 => 0.1007
activation[3] = self(0.1007) + output[5](0.1070) * -0.8000 => 0.0151
activation[4] = self(0.1669) + output[2](0.5000) * -0.6000 => -0.1331
activation[4] = self(-0.1331) + output[3](0.5847) * 1.0000 => 0.4516
activation[5] = self(-0.0424) + output[2](0.5000) * -0.8000 => -0.4424
activation[5] = self(-0.4424) + output[3](0.5847) * -0.7000 => -0.8517
activation[5] = self(-0.8517) + output[4](0.7672) * 1.0000 => -0.0845
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5847) * 1.0000 => 0.5847
activation[8] = self(0.0000) + output[5](0.1070) * 1.0000 => 0.1070
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9490)
networkOutput[2] := neuronOutput[8](0.6306)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=074 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0151) * weight[3,3](0.0100) => 0.0002
activation[4] = self(0.4516) * weight[4,4](0.7000) => 0.3161
activation[5] = self(-0.0845) * weight[5,5](0.1000) => -0.0085
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5847) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.1070) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0002) + output[1](0.5000) * 1.0000 => 0.5002
activation[3] = self(0.5002) + output[2](0.5000) * -0.8000 => 0.1002
activation[3] = self(0.1002) + output[5](0.3959) * -0.8000 => -0.2165
activation[4] = self(0.3161) + output[2](0.5000) * -0.6000 => 0.0161
activation[4] = self(0.0161) + output[3](0.5189) * 1.0000 => 0.5350
activation[5] = self(-0.0085) + output[2](0.5000) * -0.8000 => -0.4085
activation[5] = self(-0.4085) + output[3](0.5189) * -0.7000 => -0.7717
activation[5] = self(-0.7717) + output[4](0.9053) * 1.0000 => 0.1337
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5189) * 1.0000 => 0.5189
activation[8] = self(0.0000) + output[5](0.3959) * 1.0000 => 0.3959
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9305)
networkOutput[2] := neuronOutput[8](0.8786)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=075 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.2165) * weight[3,3](0.0100) => -0.0022
activation[4] = self(0.5350) * weight[4,4](0.7000) => 0.3745
activation[5] = self(0.1337) * weight[5,5](0.1000) => 0.0134
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5189) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3959) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0022) + output[1](0.5000) * 1.0000 => 0.4978
activation[3] = self(0.4978) + output[2](0.5000) * -0.8000 => 0.0978
activation[3] = self(0.0978) + output[5](0.6611) * -0.8000 => -0.4311
activation[4] = self(0.3745) + output[2](0.5000) * -0.6000 => 0.0745
activation[4] = self(0.0745) + output[3](0.2530) * 1.0000 => 0.3275
activation[5] = self(0.0134) + output[2](0.5000) * -0.8000 => -0.3866
activation[5] = self(-0.3866) + output[3](0.2530) * -0.7000 => -0.5637
activation[5] = self(-0.5637) + output[4](0.9355) * 1.0000 => 0.3718
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.2530) * 1.0000 => 0.2530
activation[8] = self(0.0000) + output[5](0.6611) * 1.0000 => 0.6611
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.7799)
networkOutput[2] := neuronOutput[8](0.9646)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=076 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.4311) * weight[3,3](0.0100) => -0.0043
activation[4] = self(0.3275) * weight[4,4](0.7000) => 0.2292
activation[5] = self(0.3718) * weight[5,5](0.1000) => 0.0372
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.2530) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6611) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0043) + output[1](0.5000) * 1.0000 => 0.4957
activation[3] = self(0.4957) + output[2](0.5000) * -0.8000 => 0.0957
activation[3] = self(0.0957) + output[5](0.8652) * -0.8000 => -0.5965
activation[4] = self(0.2292) + output[2](0.5000) * -0.6000 => -0.0708
activation[4] = self(-0.0708) + output[3](0.1038) * 1.0000 => 0.0331
activation[5] = self(0.0372) + output[2](0.5000) * -0.8000 => -0.3628
activation[5] = self(-0.3628) + output[3](0.1038) * -0.7000 => -0.4355
activation[5] = self(-0.4355) + output[4](0.8372) * 1.0000 => 0.4017
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.1038) * 1.0000 => 0.1038
activation[8] = self(0.0000) + output[5](0.8652) * 1.0000 => 0.8652
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.6269)
networkOutput[2] := neuronOutput[8](0.9870)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=077 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.5965) * weight[3,3](0.0100) => -0.0060
activation[4] = self(0.0331) * weight[4,4](0.7000) => 0.0232
activation[5] = self(0.4017) * weight[5,5](0.1000) => 0.0402
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.1038) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.8652) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0060) + output[1](0.5000) * 1.0000 => 0.4940
activation[3] = self(0.4940) + output[2](0.5000) * -0.8000 => 0.0940
activation[3] = self(0.0940) + output[5](0.8817) * -0.8000 => -0.6113
activation[4] = self(0.0232) + output[2](0.5000) * -0.6000 => -0.2768
activation[4] = self(-0.2768) + output[3](0.0482) * 1.0000 => -0.2286
activation[5] = self(0.0402) + output[2](0.5000) * -0.8000 => -0.3598
activation[5] = self(-0.3598) + output[3](0.0482) * -0.7000 => -0.3936
activation[5] = self(-0.3936) + output[4](0.5412) * 1.0000 => 0.1477
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0482) * 1.0000 => 0.0482
activation[8] = self(0.0000) + output[5](0.8817) * 1.0000 => 0.8817
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5600)
networkOutput[2] := neuronOutput[8](0.9880)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=078 --
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.6113) * weight[3,3](0.0100) => -0.0061
activation[4] = self(-0.2286) * weight[4,4](0.7000) => -0.1600
activation[5] = self(0.1477) * weight[5,5](0.1000) => 0.0148
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0482) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.8817) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0061) + output[1](0.9933) * 1.0000 => 0.9872
activation[3] = self(0.9872) + output[2](0.5000) * -0.8000 => 0.5872
activation[3] = self(0.5872) + output[5](0.6766) * -0.8000 => 0.0459
activation[4] = self(-0.1600) + output[2](0.5000) * -0.6000 => -0.4600
activation[4] = self(-0.4600) + output[3](0.0449) * 1.0000 => -0.4151
activation[5] = self(0.0148) + output[2](0.5000) * -0.8000 => -0.3852
activation[5] = self(-0.3852) + output[3](0.0449) * -0.7000 => -0.4167
activation[5] = self(-0.4167) + output[4](0.2418) * 1.0000 => -0.1749
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0449) * 1.0000 => 0.0449
activation[8] = self(0.0000) + output[5](0.6766) * 1.0000 => 0.6766
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5559)
networkOutput[2] := neuronOutput[8](0.9672)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=079 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0459) * weight[3,3](0.0100) => 0.0005
activation[4] = self(-0.4151) * weight[4,4](0.7000) => -0.2906
activation[5] = self(-0.1749) * weight[5,5](0.1000) => -0.0175
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0449) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6766) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0005) + output[1](0.5000) * 1.0000 => 0.5005
activation[3] = self(0.5005) + output[2](0.5000) * -0.8000 => 0.1005
activation[3] = self(0.1005) + output[5](0.2943) * -0.8000 => -0.1350
activation[4] = self(-0.2906) + output[2](0.5000) * -0.6000 => -0.5906
activation[4] = self(-0.5906) + output[3](0.5571) * 1.0000 => -0.0334
activation[5] = self(-0.0175) + output[2](0.5000) * -0.8000 => -0.4175
activation[5] = self(-0.4175) + output[3](0.5571) * -0.7000 => -0.8075
activation[5] = self(-0.8075) + output[4](0.1115) * 1.0000 => -0.6960
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5571) * 1.0000 => 0.5571
activation[8] = self(0.0000) + output[5](0.2943) * 1.0000 => 0.2943
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9419)
networkOutput[2] := neuronOutput[8](0.8133)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=080 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.1350) * weight[3,3](0.0100) => -0.0013
activation[4] = self(-0.0334) * weight[4,4](0.7000) => -0.0234
activation[5] = self(-0.6960) * weight[5,5](0.1000) => -0.0696
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5571) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.2943) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0013) + output[1](0.5000) * 1.0000 => 0.4987
activation[3] = self(0.4987) + output[2](0.5000) * -0.8000 => 0.0987
activation[3] = self(0.0987) + output[5](0.0299) * -0.8000 => 0.0747
activation[4] = self(-0.0234) + output[2](0.5000) * -0.6000 => -0.3234
activation[4] = self(-0.3234) + output[3](0.3374) * 1.0000 => 0.0140
activation[5] = self(-0.0696) + output[2](0.5000) * -0.8000 => -0.4696
activation[5] = self(-0.4696) + output[3](0.3374) * -0.7000 => -0.7058
activation[5] = self(-0.7058) + output[4](0.4583) * 1.0000 => -0.2475
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.3374) * 1.0000 => 0.3374
activation[8] = self(0.0000) + output[5](0.0299) * 1.0000 => 0.0299
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.8438)
networkOutput[2] := neuronOutput[8](0.5373)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=081 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0747) * weight[3,3](0.0100) => 0.0007
activation[4] = self(0.0140) * weight[4,4](0.7000) => 0.0098
activation[5] = self(-0.2475) * weight[5,5](0.1000) => -0.0247
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.3374) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0299) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0007) + output[1](0.5000) * 1.0000 => 0.5007
activation[3] = self(0.5007) + output[2](0.5000) * -0.8000 => 0.1007
activation[3] = self(0.1007) + output[5](0.2249) * -0.8000 => -0.0792
activation[4] = self(0.0098) + output[2](0.5000) * -0.6000 => -0.2902
activation[4] = self(-0.2902) + output[3](0.5924) * 1.0000 => 0.3022
activation[5] = self(-0.0247) + output[2](0.5000) * -0.8000 => -0.4247
activation[5] = self(-0.4247) + output[3](0.5924) * -0.7000 => -0.8394
activation[5] = self(-0.8394) + output[4](0.5175) * 1.0000 => -0.3219
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5924) * 1.0000 => 0.5924
activation[8] = self(0.0000) + output[5](0.2249) * 1.0000 => 0.2249
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9508)
networkOutput[2] := neuronOutput[8](0.7548)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=082 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.0792) * weight[3,3](0.0100) => -0.0008
activation[4] = self(0.3022) * weight[4,4](0.7000) => 0.2115
activation[5] = self(-0.3219) * weight[5,5](0.1000) => -0.0322
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5924) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.2249) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0008) + output[1](0.5000) * 1.0000 => 0.4992
activation[3] = self(0.4992) + output[2](0.5000) * -0.8000 => 0.0992
activation[3] = self(0.0992) + output[5](0.1667) * -0.8000 => -0.0341
activation[4] = self(0.2115) + output[2](0.5000) * -0.6000 => -0.0885
activation[4] = self(-0.0885) + output[3](0.4023) * 1.0000 => 0.3138
activation[5] = self(-0.0322) + output[2](0.5000) * -0.8000 => -0.4322
activation[5] = self(-0.4322) + output[3](0.4023) * -0.7000 => -0.7138
activation[5] = self(-0.7138) + output[4](0.8192) * 1.0000 => 0.1054
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.4023) * 1.0000 => 0.4023
activation[8] = self(0.0000) + output[5](0.1667) * 1.0000 => 0.1667
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.8820)
networkOutput[2] := neuronOutput[8](0.6971)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=083 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.0341) * weight[3,3](0.0100) => -0.0003
activation[4] = self(0.3138) * weight[4,4](0.7000) => 0.2197
activation[5] = self(0.1054) * weight[5,5](0.1000) => 0.0105
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.4023) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.1667) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0003) + output[1](0.5000) * 1.0000 => 0.4997
activation[3] = self(0.4997) + output[2](0.5000) * -0.8000 => 0.0997
activation[3] = self(0.0997) + output[5](0.6287) * -0.8000 => -0.4033
activation[4] = self(0.2197) + output[2](0.5000) * -0.6000 => -0.0803
activation[4] = self(-0.0803) + output[3](0.4575) * 1.0000 => 0.3771
activation[5] = self(0.0105) + output[2](0.5000) * -0.8000 => -0.3895
activation[5] = self(-0.3895) + output[3](0.4575) * -0.7000 => -0.7097
activation[5] = self(-0.7097) + output[4](0.8277) * 1.0000 => 0.1180
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.4575) * 1.0000 => 0.4575
activation[8] = self(0.0000) + output[5](0.6287) * 1.0000 => 0.6287
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9078)
networkOutput[2] := neuronOutput[8](0.9587)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=084 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.4033) * weight[3,3](0.0100) => -0.0040
activation[4] = self(0.3771) * weight[4,4](0.7000) => 0.2640
activation[5] = self(0.1180) * weight[5,5](0.1000) => 0.0118
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.4575) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6287) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0040) + output[1](0.5000) * 1.0000 => 0.4960
activation[3] = self(0.4960) + output[2](0.5000) * -0.8000 => 0.0960
activation[3] = self(0.0960) + output[5](0.6433) * -0.8000 => -0.4187
activation[4] = self(0.2640) + output[2](0.5000) * -0.6000 => -0.0360
activation[4] = self(-0.0360) + output[3](0.1175) * 1.0000 => 0.0815
activation[5] = self(0.0118) + output[2](0.5000) * -0.8000 => -0.3882
activation[5] = self(-0.3882) + output[3](0.1175) * -0.7000 => -0.4704
activation[5] = self(-0.4704) + output[4](0.8683) * 1.0000 => 0.3978
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.1175) * 1.0000 => 0.1175
activation[8] = self(0.0000) + output[5](0.6433) * 1.0000 => 0.6433
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.6428)
networkOutput[2] := neuronOutput[8](0.9615)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=085 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.4187) * weight[3,3](0.0100) => -0.0042
activation[4] = self(0.0815) * weight[4,4](0.7000) => 0.0570
activation[5] = self(0.3978) * weight[5,5](0.1000) => 0.0398
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.1175) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6433) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0042) + output[1](0.5000) * 1.0000 => 0.4958
activation[3] = self(0.4958) + output[2](0.5000) * -0.8000 => 0.0958
activation[3] = self(0.0958) + output[5](0.8797) * -0.8000 => -0.6079
activation[4] = self(0.0570) + output[2](0.5000) * -0.6000 => -0.2430
activation[4] = self(-0.2430) + output[3](0.1097) * 1.0000 => -0.1333
activation[5] = self(0.0398) + output[2](0.5000) * -0.8000 => -0.3602
activation[5] = self(-0.3602) + output[3](0.1097) * -0.7000 => -0.4370
activation[5] = self(-0.4370) + output[4](0.6004) * 1.0000 => 0.1634
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.1097) * 1.0000 => 0.1097
activation[8] = self(0.0000) + output[5](0.8797) * 1.0000 => 0.8797
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.6338)
networkOutput[2] := neuronOutput[8](0.9879)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=086 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.6079) * weight[3,3](0.0100) => -0.0061
activation[4] = self(-0.1333) * weight[4,4](0.7000) => -0.0933
activation[5] = self(0.1634) * weight[5,5](0.1000) => 0.0163
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.1097) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.8797) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0061) + output[1](0.5000) * 1.0000 => 0.4939
activation[3] = self(0.4939) + output[2](0.5000) * -0.8000 => 0.0939
activation[3] = self(0.0939) + output[5](0.6936) * -0.8000 => -0.4610
activation[4] = self(-0.0933) + output[2](0.5000) * -0.6000 => -0.3933
activation[4] = self(-0.3933) + output[3](0.0457) * 1.0000 => -0.3476
activation[5] = self(0.0163) + output[2](0.5000) * -0.8000 => -0.3837
activation[5] = self(-0.3837) + output[3](0.0457) * -0.7000 => -0.4156
activation[5] = self(-0.4156) + output[4](0.3393) * 1.0000 => -0.0763
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0457) * 1.0000 => 0.0457
activation[8] = self(0.0000) + output[5](0.6936) * 1.0000 => 0.6936
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5568)
networkOutput[2] := neuronOutput[8](0.9698)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=087 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.4610) * weight[3,3](0.0100) => -0.0046
activation[4] = self(-0.3476) * weight[4,4](0.7000) => -0.2433
activation[5] = self(-0.0763) * weight[5,5](0.1000) => -0.0076
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0457) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6936) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0046) + output[1](0.5000) * 1.0000 => 0.4954
activation[3] = self(0.4954) + output[2](0.5000) * -0.8000 => 0.0954
activation[3] = self(0.0954) + output[5](0.4058) * -0.8000 => -0.2292
activation[4] = self(-0.2433) + output[2](0.5000) * -0.6000 => -0.5433
activation[4] = self(-0.5433) + output[3](0.0907) * 1.0000 => -0.4526
activation[5] = self(-0.0076) + output[2](0.5000) * -0.8000 => -0.4076
activation[5] = self(-0.4076) + output[3](0.0907) * -0.7000 => -0.4711
activation[5] = self(-0.4711) + output[4](0.1496) * 1.0000 => -0.3216
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0907) * 1.0000 => 0.0907
activation[8] = self(0.0000) + output[5](0.4058) * 1.0000 => 0.4058
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.6115)
networkOutput[2] := neuronOutput[8](0.8838)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=088 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.2292) * weight[3,3](0.0100) => -0.0023
activation[4] = self(-0.4526) * weight[4,4](0.7000) => -0.3168
activation[5] = self(-0.3216) * weight[5,5](0.1000) => -0.0322
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0907) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.4058) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0023) + output[1](0.5000) * 1.0000 => 0.4977
activation[3] = self(0.4977) + output[2](0.5000) * -0.8000 => 0.0977
activation[3] = self(0.0977) + output[5](0.1669) * -0.8000 => -0.0358
activation[4] = self(-0.3168) + output[2](0.5000) * -0.6000 => -0.6168
activation[4] = self(-0.6168) + output[3](0.2412) * 1.0000 => -0.3756
activation[5] = self(-0.0322) + output[2](0.5000) * -0.8000 => -0.4322
activation[5] = self(-0.4322) + output[3](0.2412) * -0.7000 => -0.6010
activation[5] = self(-0.6010) + output[4](0.0942) * 1.0000 => -0.5068
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.2412) * 1.0000 => 0.2412
activation[8] = self(0.0000) + output[5](0.1669) * 1.0000 => 0.1669
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.7696)
networkOutput[2] := neuronOutput[8](0.6973)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=089 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.0358) * weight[3,3](0.0100) => -0.0004
activation[4] = self(-0.3756) * weight[4,4](0.7000) => -0.2629
activation[5] = self(-0.5068) * weight[5,5](0.1000) => -0.0507
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.2412) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.1669) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0004) + output[1](0.5000) * 1.0000 => 0.4996
activation[3] = self(0.4996) + output[2](0.5000) * -0.8000 => 0.0996
activation[3] = self(0.0996) + output[5](0.0735) * -0.8000 => 0.0408
activation[4] = self(-0.2629) + output[2](0.5000) * -0.6000 => -0.5629
activation[4] = self(-0.5629) + output[3](0.4554) * 1.0000 => -0.1076
activation[5] = self(-0.0507) + output[2](0.5000) * -0.8000 => -0.4507
activation[5] = self(-0.4507) + output[3](0.4554) * -0.7000 => -0.7694
activation[5] = self(-0.7694) + output[4](0.1326) * 1.0000 => -0.6368
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.4554) * 1.0000 => 0.4554
activation[8] = self(0.0000) + output[5](0.0735) * 1.0000 => 0.0735
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9069)
networkOutput[2] := neuronOutput[8](0.5909)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=090 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0408) * weight[3,3](0.0100) => 0.0004
activation[4] = self(-0.1076) * weight[4,4](0.7000) => -0.0753
activation[5] = self(-0.6368) * weight[5,5](0.1000) => -0.0637
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.4554) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0735) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0004) + output[1](0.5000) * 1.0000 => 0.5004
activation[3] = self(0.5004) + output[2](0.5000) * -0.8000 => 0.1004
activation[3] = self(0.1004) + output[5](0.0398) * -0.8000 => 0.0686
activation[4] = self(-0.0753) + output[2](0.5000) * -0.6000 => -0.3753
activation[4] = self(-0.3753) + output[3](0.5509) * 1.0000 => 0.1756
activation[5] = self(-0.0637) + output[2](0.5000) * -0.8000 => -0.4637
activation[5] = self(-0.4637) + output[3](0.5509) * -0.7000 => -0.8493
activation[5] = self(-0.8493) + output[4](0.3687) * 1.0000 => -0.4806
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5509) * 1.0000 => 0.5509
activation[8] = self(0.0000) + output[5](0.0398) * 1.0000 => 0.0398
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9402)
networkOutput[2] := neuronOutput[8](0.5495)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=091 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0686) * weight[3,3](0.0100) => 0.0007
activation[4] = self(0.1756) * weight[4,4](0.7000) => 0.1229
activation[5] = self(-0.4806) * weight[5,5](0.1000) => -0.0481
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5509) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0398) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0007) + output[1](0.5000) * 1.0000 => 0.5007
activation[3] = self(0.5007) + output[2](0.5000) * -0.8000 => 0.1007
activation[3] = self(0.1007) + output[5](0.0829) * -0.8000 => 0.0343
activation[4] = self(0.1229) + output[2](0.5000) * -0.6000 => -0.1771
activation[4] = self(-0.1771) + output[3](0.5849) * 1.0000 => 0.4078
activation[5] = self(-0.0481) + output[2](0.5000) * -0.8000 => -0.4481
activation[5] = self(-0.4481) + output[3](0.5849) * -0.7000 => -0.8575
activation[5] = self(-0.8575) + output[4](0.7064) * 1.0000 => -0.1511
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5849) * 1.0000 => 0.5849
activation[8] = self(0.0000) + output[5](0.0829) * 1.0000 => 0.0829
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9490)
networkOutput[2] := neuronOutput[8](0.6022)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=092 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0343) * weight[3,3](0.0100) => 0.0003
activation[4] = self(0.4078) * weight[4,4](0.7000) => 0.2855
activation[5] = self(-0.1511) * weight[5,5](0.1000) => -0.0151
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5849) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0829) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0003) + output[1](0.5000) * 1.0000 => 0.5003
activation[3] = self(0.5003) + output[2](0.5000) * -0.8000 => 0.1003
activation[3] = self(0.1003) + output[5](0.3196) * -0.8000 => -0.1553
activation[4] = self(0.2855) + output[2](0.5000) * -0.6000 => -0.0145
activation[4] = self(-0.0145) + output[3](0.5428) * 1.0000 => 0.5283
activation[5] = self(-0.0151) + output[2](0.5000) * -0.8000 => -0.4151
activation[5] = self(-0.4151) + output[3](0.5428) * -0.7000 => -0.7951
activation[5] = self(-0.7951) + output[4](0.8848) * 1.0000 => 0.0898
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5428) * 1.0000 => 0.5428
activation[8] = self(0.0000) + output[5](0.3196) * 1.0000 => 0.3196
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9378)
networkOutput[2] := neuronOutput[8](0.8317)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=093 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.1553) * weight[3,3](0.0100) => -0.0016
activation[4] = self(0.5283) * weight[4,4](0.7000) => 0.3698
activation[5] = self(0.0898) * weight[5,5](0.1000) => 0.0090
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5428) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3196) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0016) + output[1](0.5000) * 1.0000 => 0.4984
activation[3] = self(0.4984) + output[2](0.5000) * -0.8000 => 0.0984
activation[3] = self(0.0984) + output[5](0.6103) * -0.8000 => -0.3898
activation[4] = self(0.3698) + output[2](0.5000) * -0.6000 => 0.0698
activation[4] = self(0.0698) + output[3](0.3151) * 1.0000 => 0.3848
activation[5] = self(0.0090) + output[2](0.5000) * -0.8000 => -0.3910
activation[5] = self(-0.3910) + output[3](0.3151) * -0.7000 => -0.6116
activation[5] = self(-0.6116) + output[4](0.9335) * 1.0000 => 0.3219
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.3151) * 1.0000 => 0.3151
activation[8] = self(0.0000) + output[5](0.6103) * 1.0000 => 0.6103
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.8285)
networkOutput[2] := neuronOutput[8](0.9549)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=094 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.3898) * weight[3,3](0.0100) => -0.0039
activation[4] = self(0.3848) * weight[4,4](0.7000) => 0.2694
activation[5] = self(0.3219) * weight[5,5](0.1000) => 0.0322
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.3151) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.6103) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0039) + output[1](0.5000) * 1.0000 => 0.4961
activation[3] = self(0.4961) + output[2](0.5000) * -0.8000 => 0.0961
activation[3] = self(0.0961) + output[5](0.8334) * -0.8000 => -0.5706
activation[4] = self(0.2694) + output[2](0.5000) * -0.6000 => -0.0306
activation[4] = self(-0.0306) + output[3](0.1246) * 1.0000 => 0.0940
activation[5] = self(0.0322) + output[2](0.5000) * -0.8000 => -0.3678
activation[5] = self(-0.3678) + output[3](0.1246) * -0.7000 => -0.4551
activation[5] = self(-0.4551) + output[4](0.8726) * 1.0000 => 0.4175
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.1246) * 1.0000 => 0.1246
activation[8] = self(0.0000) + output[5](0.8334) * 1.0000 => 0.8334
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.6510)
networkOutput[2] := neuronOutput[8](0.9847)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=095 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.5706) * weight[3,3](0.0100) => -0.0057
activation[4] = self(0.0940) * weight[4,4](0.7000) => 0.0658
activation[5] = self(0.4175) * weight[5,5](0.1000) => 0.0418
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.1246) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.8334) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0057) + output[1](0.5000) * 1.0000 => 0.4943
activation[3] = self(0.4943) + output[2](0.5000) * -0.8000 => 0.0943
activation[3] = self(0.0943) + output[5](0.8897) * -0.8000 => -0.6175
activation[4] = self(0.0658) + output[2](0.5000) * -0.6000 => -0.2342
activation[4] = self(-0.2342) + output[3](0.0545) * 1.0000 => -0.1796
activation[5] = self(0.0418) + output[2](0.5000) * -0.8000 => -0.3582
activation[5] = self(-0.3582) + output[3](0.0545) * -0.7000 => -0.3964
activation[5] = self(-0.3964) + output[4](0.6154) * 1.0000 => 0.2190
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0545) * 1.0000 => 0.0545
activation[8] = self(0.0000) + output[5](0.8897) * 1.0000 => 0.8897
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5677)
networkOutput[2] := neuronOutput[8](0.9884)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=096 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.6175) * weight[3,3](0.0100) => -0.0062
activation[4] = self(-0.1796) * weight[4,4](0.7000) => -0.1258
activation[5] = self(0.2190) * weight[5,5](0.1000) => 0.0219
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0545) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.8897) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0062) + output[1](0.5000) * 1.0000 => 0.4938
activation[3] = self(0.4938) + output[2](0.5000) * -0.8000 => 0.0938
activation[3] = self(0.0938) + output[5](0.7493) * -0.8000 => -0.5056
activation[4] = self(-0.1258) + output[2](0.5000) * -0.6000 => -0.4258
activation[4] = self(-0.4258) + output[3](0.0436) * 1.0000 => -0.3821
activation[5] = self(0.0219) + output[2](0.5000) * -0.8000 => -0.3781
activation[5] = self(-0.3781) + output[3](0.0436) * -0.7000 => -0.4086
activation[5] = self(-0.4086) + output[4](0.2894) * 1.0000 => -0.1192
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0436) * 1.0000 => 0.0436
activation[8] = self(0.0000) + output[5](0.7493) * 1.0000 => 0.7493
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5543)
networkOutput[2] := neuronOutput[8](0.9769)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=097 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.5056) * weight[3,3](0.0100) => -0.0051
activation[4] = self(-0.3821) * weight[4,4](0.7000) => -0.2675
activation[5] = self(-0.1192) * weight[5,5](0.1000) => -0.0119
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0436) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.7493) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0051) + output[1](0.5000) * 1.0000 => 0.4949
activation[3] = self(0.4949) + output[2](0.5000) * -0.8000 => 0.0949
activation[3] = self(0.0949) + output[5](0.3552) * -0.8000 => -0.1892
activation[4] = self(-0.2675) + output[2](0.5000) * -0.6000 => -0.5675
activation[4] = self(-0.5675) + output[3](0.0739) * 1.0000 => -0.4936
activation[5] = self(-0.0119) + output[2](0.5000) * -0.8000 => -0.4119
activation[5] = self(-0.4119) + output[3](0.0739) * -0.7000 => -0.4637
activation[5] = self(-0.4637) + output[4](0.1289) * 1.0000 => -0.3347
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.0739) * 1.0000 => 0.0739
activation[8] = self(0.0000) + output[5](0.3552) * 1.0000 => 0.3552
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.5913)
networkOutput[2] := neuronOutput[8](0.8552)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=098 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.1892) * weight[3,3](0.0100) => -0.0019
activation[4] = self(-0.4936) * weight[4,4](0.7000) => -0.3455
activation[5] = self(-0.3347) * weight[5,5](0.1000) => -0.0335
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.0739) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.3552) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0019) + output[1](0.5000) * 1.0000 => 0.4981
activation[3] = self(0.4981) + output[2](0.5000) * -0.8000 => 0.0981
activation[3] = self(0.0981) + output[5](0.1579) * -0.8000 => -0.0282
activation[4] = self(-0.3455) + output[2](0.5000) * -0.6000 => -0.6455
activation[4] = self(-0.6455) + output[3](0.2796) * 1.0000 => -0.3659
activation[5] = self(-0.0335) + output[2](0.5000) * -0.8000 => -0.4335
activation[5] = self(-0.4335) + output[3](0.2796) * -0.7000 => -0.6292
activation[5] = self(-0.6292) + output[4](0.0781) * 1.0000 => -0.5511
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.2796) * 1.0000 => 0.2796
activation[8] = self(0.0000) + output[5](0.1579) * 1.0000 => 0.1579
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.8019)
networkOutput[2] := neuronOutput[8](0.6878)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=099 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(-0.0282) * weight[3,3](0.0100) => -0.0003
activation[4] = self(-0.3659) * weight[4,4](0.7000) => -0.2561
activation[5] = self(-0.5511) * weight[5,5](0.1000) => -0.0551
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.2796) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.1579) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0003) + output[1](0.5000) * 1.0000 => 0.4997
activation[3] = self(0.4997) + output[2](0.5000) * -0.8000 => 0.0997
activation[3] = self(0.0997) + output[5](0.0598) * -0.8000 => 0.0519
activation[4] = self(-0.2561) + output[2](0.5000) * -0.6000 => -0.5561
activation[4] = self(-0.5561) + output[3](0.4648) * 1.0000 => -0.0913
activation[5] = self(-0.0551) + output[2](0.5000) * -0.8000 => -0.4551
activation[5] = self(-0.4551) + output[3](0.4648) * -0.7000 => -0.7804
activation[5] = self(-0.7804) + output[4](0.1383) * 1.0000 => -0.6421
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.4648) * 1.0000 => 0.4648
activation[8] = self(0.0000) + output[5](0.0598) * 1.0000 => 0.0598
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9108)
networkOutput[2] := neuronOutput[8](0.5742)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=100 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0519) * weight[3,3](0.0100) => 0.0005
activation[4] = self(-0.0913) * weight[4,4](0.7000) => -0.0639
activation[5] = self(-0.6421) * weight[5,5](0.1000) => -0.0642
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.4648) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0598) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0005) + output[1](0.5000) * 1.0000 => 0.5005
activation[3] = self(0.5005) + output[2](0.5000) * -0.8000 => 0.1005
activation[3] = self(0.1005) + output[5](0.0388) * -0.8000 => 0.0695
activation[4] = self(-0.0639) + output[2](0.5000) * -0.6000 => -0.3639
activation[4] = self(-0.3639) + output[3](0.5645) * 1.0000 => 0.2006
activation[5] = self(-0.0642) + output[2](0.5000) * -0.8000 => -0.4642
activation[5] = self(-0.4642) + output[3](0.5645) * -0.7000 => -0.8594
activation[5] = self(-0.8594) + output[4](0.3878) * 1.0000 => -0.4716
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5645) * 1.0000 => 0.5645
activation[8] = self(0.0000) + output[5](0.0388) * 1.0000 => 0.0388
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9439)
networkOutput[2] := neuronOutput[8](0.5483)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=101 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0695) * weight[3,3](0.0100) => 0.0007
activation[4] = self(0.2006) * weight[4,4](0.7000) => 0.1404
activation[5] = self(-0.4716) * weight[5,5](0.1000) => -0.0472
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5645) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0388) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0007) + output[1](0.5000) * 1.0000 => 0.5007
activation[3] = self(0.5007) + output[2](0.5000) * -0.8000 => 0.1007
activation[3] = self(0.1007) + output[5](0.0864) * -0.8000 => 0.0315
activation[4] = self(0.1404) + output[2](0.5000) * -0.6000 => -0.1596
activation[4] = self(-0.1596) + output[3](0.5860) * 1.0000 => 0.4264
activation[5] = self(-0.0472) + output[2](0.5000) * -0.8000 => -0.4472
activation[5] = self(-0.4472) + output[3](0.5860) * -0.7000 => -0.8574
activation[5] = self(-0.8574) + output[4](0.7316) * 1.0000 => -0.1258
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5860) * 1.0000 => 0.5860
activation[8] = self(0.0000) + output[5](0.0864) * 1.0000 => 0.0864
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9493)
networkOutput[2] := neuronOutput[8](0.6064)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = weight_total[7](1.0000) * weight[3,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
-- t=102 --
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.0000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.0000) => 0.0000
activation[3] = self(0.0315) * weight[3,3](0.0100) => 0.0003
activation[4] = self(0.4264) * weight[4,4](0.7000) => 0.2985
activation[5] = self(-0.1258) * weight[5,5](0.1000) => -0.0126
activation[6] = self(0.5000) * weight[6,6](0.0000) => 0.0000
activation[7] = self(0.5860) * weight[7,7](0.0000) => 0.0000
activation[8] = self(0.0864) * weight[8,8](0.0000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.0000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0003) + output[1](0.5000) * 1.0000 => 0.5003
activation[3] = self(0.5003) + output[2](0.5000) * -0.8000 => 0.1003
activation[3] = self(0.1003) + output[5](0.3478) * -0.8000 => -0.1779
activation[4] = self(0.2985) + output[2](0.5000) * -0.6000 => -0.0015
activation[4] = self(-0.0015) + output[3](0.5394) * 1.0000 => 0.5378
activation[5] = self(-0.0126) + output[2](0.5000) * -0.8000 => -0.4126
activation[5] = self(-0.4126) + output[3](0.5394) * -0.7000 => -0.7901
activation[5] = self(-0.7901) + output[4](0.8940) * 1.0000 => 0.1039
activation[6] = self(0.0000) + output[2](0.5000) * 1.0000 => 0.5000
activation[7] = self(0.0000) + output[3](0.5394) * 1.0000 => 0.5394
activation[8] = self(0.0000) + output[5](0.3478) * 1.0000 => 0.3478
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
4. setNetworkOutput()
networkOutput[1] := neuronOutput[7](0.9368)
networkOutput[2] := neuronOutput[8](0.8506)
networkOutput[3] := neuronOutput[9](0.5000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[3,4] = weight_total[4](1.0000) * weight[3,4](1.0000) / sum(1.0000) => 1.0000
weight[4,5] = weight_total[5](1.0000) * weight[4,5](1.0000) / sum(1.0000) => 1.0000
weight[2,6] = weight_total[6](1.0000) * weight[2,6](1.0000) / sum(1.0000) => 1.0000
weight[3,7] = we